import json
import shutil
import html
import tempfile
from pathlib import Path
from urllib.request import urlretrieve
from typing import List, Dict, Any
import io
import sys
import re
import mimetypes
import base64

from flask import Flask, request, jsonify, send_from_directory, send_file
from flask_cors import CORS

try:
    import torch  # type: ignore
except ImportError:
    torch = None

try:
    import whisper  # type: ignore
except ImportError:  # Whisper is optional for local/offline use
    whisper = None

# æ–‡æœ¬å¤„ç†åº“
try:
    import PyPDF2  # type: ignore
except ImportError:
    PyPDF2 = None

try:
    import epub  # type: ignore
except ImportError:
    epub = None

try:
    from ebooklib import epub as ebooklib_epub  # type: ignore
except ImportError:
    ebooklib_epub = None

try:
    import pymorphy2  # type: ignore
except ImportError:
    pymorphy2 = None

# pymorphy2 å…¼å®¹æ€§è¡¥ä¸ï¼ˆä¿®å¤ Python 3.11+ çš„ getargspec é—®é¢˜ï¼‰
if pymorphy2 is not None:
    try:
        import inspect
        if not hasattr(inspect, 'getargspec'):
            def getargspec(func):
                try:
                    spec = inspect.getfullargspec(func)
                    from collections import namedtuple
                    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
                    return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)
                except Exception:
                    from collections import namedtuple
                    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
                    return ArgSpec([], None, None, None)
            inspect.getargspec = getargspec
            print("âœ“ pymorphy2 å…¼å®¹æ€§è¡¥ä¸å·²åŠ è½½")
    except Exception as e:
        print(f"âš ï¸ pymorphy2 å…¼å®¹æ€§è¡¥ä¸åŠ è½½å¤±è´¥: {e}")

app = Flask(
    __name__,
    static_folder=str(Path(__file__).parent.parent / "static"),
    static_url_path="/static"
)
CORS(app)
# å…è®¸ä¸Šä¼ è¾ƒå¤§æ–‡ä»¶ï¼ˆé»˜è®¤æ— é™åˆ¶ï¼Œè¿™é‡Œè®¾ç½®ä¸Šé™ 512MB ä»¥é˜²æ„å¤– 413ï¼‰
app.config["MAX_CONTENT_LENGTH"] = 512 * 1024 * 1024

# --- User/Data/Config Directories -----------------------------------------
USER_DATA_DIR = Path(__file__).parent.parent / "user_data"
CONFIG_DIR = Path(__file__).parent.parent / "config"
MODELS_DIR = Path(__file__).parent.parent / "models"
USER_DATA_DIR.mkdir(exist_ok=True)

def get_user_file_path(filename: str, subdir: str = "") -> Path:
    """è·å–ç”¨æˆ·æ•°æ®æ–‡ä»¶è·¯å¾„"""
    if subdir:
        target_dir = USER_DATA_DIR / subdir
        target_dir.mkdir(exist_ok=True)
        return target_dir / filename
    return USER_DATA_DIR / filename


# --- Whisper helpers -------------------------------------------------------

_model_cache = None
_ffmpeg_available = None
_device = None
_transcribe_progress = {"status": "", "progress": 0}

def get_device():
    """Get the device to use for inference (GPU or CPU)."""
    global _device
    if _device is not None:
        return _device
    
    if torch is not None and torch.cuda.is_available():
        _device = "cuda"
        print(f"ğŸš€ CUDA GPU available! Using GPU for inference.")
        print(f"   Device: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        _device = "cpu"
        if torch is None:
            print("âš ï¸  PyTorch not installed, using CPU")
        else:
            print("â„¹ï¸  No CUDA GPU available, using CPU")
    
    return _device

def check_ffmpeg():
    """Check if FFmpeg is available in the system."""
    global _ffmpeg_available
    if _ffmpeg_available is not None:
        return _ffmpeg_available
    _ffmpeg_available = shutil.which("ffmpeg") is not None
    if not _ffmpeg_available:
        print("âš ï¸  WARNING: FFmpeg not found! Audio transcription will fail.")
        print("   Please install FFmpeg: https://ffmpeg.org/download.html")
        print("   Windows: winget install Gyan.FFmpeg")
    return _ffmpeg_available

def get_model():
    """Lazily load Whisper model; return None if unavailable."""
    global _model_cache
    if _model_cache is not None:
        return _model_cache
    if whisper is None:
        return None
    
    device = get_device()
    
    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒ active_model.txt æŒ‡å®šï¼‰
    model_files = sorted(MODELS_DIR.glob("*.pt")) if MODELS_DIR.exists() else []
    
    if model_files:
        # ä½¿ç”¨æ¿€æ´»çš„æ¨¡å‹ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ª
        active_path = CONFIG_DIR / "active_model.txt"
        chosen = None
        if active_path.exists():
            name = active_path.read_text().strip()
            candidate = MODELS_DIR / name
            if candidate.exists():
                chosen = candidate
        model_path = chosen if chosen is not None else model_files[0]
        print(f"Loading local model: {model_path.name}")
        _model_cache = whisper.load_model(str(model_path), device=device)
    else:
        # å›é€€åˆ°è‡ªåŠ¨ä¸‹è½½æ¨¡å¼
        env_path = CONFIG_DIR / ".env"
        model_name = env_path.read_text().strip() if env_path.exists() else "base"
        print(f"Downloading model: {model_name}")
        _model_cache = whisper.load_model(model_name, device=device)
    
    return _model_cache

# --- Model management APIs -------------------------------------------------

MODEL_URLS = {
    "tiny": "https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt",
    "base": "https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt",
    "small": "https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt",
    "medium": "https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt",
    "large": "https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large.pt",
}

def list_local_models():
    items = []
    if MODELS_DIR.exists():
        for m in MODELS_DIR.glob("*.pt"):
            items.append({
                "filename": m.name,
                "size_mb": round(m.stat().st_size / (1024 * 1024), 1)
            })
    return items

@app.get('/api/models/list')
def api_models_list():
    local = list_local_models()
    current = None
    active_path = CONFIG_DIR / "active_model.txt"
    if active_path.exists():
        current = active_path.read_text().strip()
    elif local:
        current = local[0]["filename"]
    else:
        env_path = CONFIG_DIR / ".env"
        current = env_path.read_text().strip() if env_path.exists() else None
    return {"status": "success", "local": local, "current": current, "canSwitch": len(local) > 1}

@app.post('/api/models/download')
def api_models_download():
    data = request.get_json(silent=True) or {}
    name = str(data.get('name', 'base')).lower()
    if name not in MODEL_URLS:
        return {"status": "error", "message": "unknown model"}, 400
    MODELS_DIR.mkdir(exist_ok=True)
    target = MODELS_DIR / f"{name}.pt"
    try:
        url = MODEL_URLS[name]
        urlretrieve(url, target)
        return {"status": "success", "filename": target.name}
    except Exception as e:
        return {"status": "error", "message": str(e)}, 500

@app.post('/api/models/set_active')
def api_models_set_active():
    data = request.get_json(silent=True) or {}
    filename = data.get('filename')
    if not filename:
        return {"status": "error", "message": "filename required"}, 400
    candidate = MODELS_DIR / filename
    if not candidate.exists():
        return {"status": "error", "message": "file not found"}, 404
    CONFIG_DIR.mkdir(exist_ok=True)
    (CONFIG_DIR / 'active_model.txt').write_text(filename)
    # æ¸…ç†ç¼“å­˜ä»¥ä¾¿ä¸‹æ¬¡åŠ è½½æ–°æ¨¡å‹
    global _model_cache
    _model_cache = None
    return {"status": "success", "active": filename}


def run_whisper_transcribe(tmp_path: Path, language: str | None = None) -> Dict[str, Any]:
    global _transcribe_progress
    model = get_model()
    if model is None:
        return {"text": "", "segments": []}
    if not check_ffmpeg():
        raise RuntimeError(
            "FFmpeg is not installed. Please install FFmpeg to use audio transcription. "
            "See INSTALL_FFMPEG.md for installation instructions."
        )
    
    print(f"ğŸ”„ Starting transcription: {tmp_path.name}")
    _transcribe_progress = {"status": "åŠ è½½ä¸­...", "progress": 5}
    
    # æ•è·è¿›åº¦è¾“å‡º
    old_stdout = sys.stdout
    progress_capture = io.StringIO()
    
    try:
        sys.stdout = progress_capture
        result = model.transcribe(str(tmp_path), language=language, verbose=False)
    finally:
        sys.stdout = old_stdout
    
    # è§£æè¿›åº¦ä¿¡æ¯
    progress_output = progress_capture.getvalue()
    if "Detected language" in progress_output:
        for line in progress_output.split('\n'):
            if "Detected language" in line:
                _transcribe_progress["detected_lang"] = line.strip()
                print(f"ğŸŒ {line.strip()}")
    
    _transcribe_progress["status"] = f"å¤„ç†ä¸­ ({len(result.get('segments', []))} ä¸ªç‰‡æ®µ)"
    _transcribe_progress["progress"] = 90
    
    print(f"âœ… Transcription complete: {len(result.get('segments', []))} segments")
    
    segments = [
        {
            "start": float(seg.get("start", 0)),
            "end": float(seg.get("end", 0)),
            "text": seg.get("text", "").strip(),
        }
        for seg in result.get("segments", [])
    ]
    
    _transcribe_progress["status"] = "å®Œæˆ"
    _transcribe_progress["progress"] = 100
    
    return {"text": result.get("text", ""), "segments": segments}


# --- Document Processing Helpers -------------------------------------------

def extract_text_from_pdf(file_path: str, max_pages: int = 50) -> str:
    """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬å¹¶ä»¥åˆ†é¡µHTMLå½¢å¼è¿”å›ï¼Œä¿æŒåŸæœ‰æ’ç‰ˆï¼ˆé€é¡µå±•ç¤ºï¼‰
    
    å‚æ•°:
        file_path: PDFæ–‡ä»¶è·¯å¾„
        max_pages: æœ€å¤§æå–é¡µæ•°ï¼ˆé˜²æ­¢å¤§å‹PDFå†…å­˜æº¢å‡ºï¼‰ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶
    """
    if PyPDF2 is None:
        raise ImportError("PyPDF2 not installed. Please install it: pip install PyPDF2")
    
    pages_html = []
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            
            # é™åˆ¶å¤„ç†é¡µæ•°ï¼Œé˜²æ­¢å¤§å‹PDFå¯¼è‡´å†…å­˜æº¢å‡º
            if max_pages > 0 and total_pages > max_pages:
                print(f"âš ï¸ PDFè¿‡å¤§ ({total_pages}é¡µ)ï¼Œä»…æå–å‰{max_pages}é¡µ")
                total_pages = max_pages
            
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text() or ""
                # ä½¿ç”¨ <pre> ä¿ç•™æ¢è¡Œä¸ç©ºæ ¼ï¼ŒåŒ…è£…é¡µå®¹å™¨æ–¹ä¾¿åˆ†é¡µå±•ç¤º
                escaped = html.escape(text)
                page_html = (
                    f'<div class="pdf-page">'
                    f'<div class="page-number">ç¬¬ {page_num + 1} é¡µ / {total_pages}</div>'
                    f'<pre>{escaped}</pre>'
                    f'</div>'
                )
                pages_html.append(page_html)
    except Exception as e:
        raise Exception(f"PDFæå–é”™è¯¯: {str(e)}")
    
    return '\n'.join(pages_html)


def extract_text_from_epub(file_path: str) -> str:
    """ä»EPUBæ–‡ä»¶æå–æ–‡æœ¬å’ŒHTMLå†…å®¹"""
    if ebooklib_epub is None:
        raise ImportError("ebooklib not installed. Please install it: pip install ebooklib")
    
    text_content = []
    try:
        book = ebooklib_epub.read_epub(file_path)
        
        # é¦–å…ˆæå–å¹¶ä¿å­˜æ‰€æœ‰å›¾ç‰‡èµ„æº
        import hashlib
        import io
        
        # ç¡®ä¿USER_DATA_DIRå¯è§
        global USER_DATA_DIR
        
        print(f"DEBUG: USER_DATA_DIR = {USER_DATA_DIR}")
        epub_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
        print(f"DEBUG: epub_hash = {epub_hash}")
        
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ä¿å­˜å›¾ç‰‡åˆ°user_data\readingsç›®å½•
        images_dir = USER_DATA_DIR / "readings" / "reading_images" / epub_hash
        print(f"DEBUG: images_dir = {images_dir}")
        
        try:
            images_dir.mkdir(parents=True, exist_ok=True)
            print(f"DEBUG: å›¾ç‰‡ç›®å½•åˆ›å»ºæˆåŠŸ: {images_dir}")
        except Exception as e:
            print(f"DEBUG: å›¾ç‰‡ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
            raise
        
        image_mapping = {}  # æ˜ å°„ï¼šæ–‡ä»¶å -> æ–°URL
        
        # æ”¶é›†æ‰€æœ‰é¡¹ç›®ç”¨äºè°ƒè¯•
        all_items = list(book.get_items())
        print(f"âœ“ EPUBå…±æœ‰ {len(all_items)} ä¸ªé¡¹ç›®")
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        for item in all_items:
            item_type = item.get_type()
            item_name = item.get_name() if hasattr(item, 'get_name') else str(item)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡èµ„æº
            is_image = False
            
            # æ–¹æ³•1ï¼šæ£€æŸ¥ ITEM_IMAGE å¸¸é‡
            if hasattr(ebooklib_epub, 'ITEM_IMAGE'):
                is_image = item_type == ebooklib_epub.ITEM_IMAGE
            else:
                # æ–¹æ³•2ï¼šæ£€æŸ¥å…ƒç»„æ ¼å¼æˆ–æ•´æ•°å€¼
                if isinstance(item_type, tuple):
                    is_image = item_type[0] == 3  # IMAGE = (3, 'DOCUMENT_IMAGE')
                elif isinstance(item_type, int):
                    is_image = item_type == 3
            
            # æ–¹æ³•3ï¼šå¦‚æœåç§°åŒ…å«å›¾ç‰‡æ‰©å±•åï¼Œä¹Ÿè®¤ä¸ºæ˜¯å›¾ç‰‡
            if not is_image and item_name and any(item_name.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp']):
                is_image = True
                print(f"  âœ“ é€šè¿‡æ‰©å±•åæ£€æµ‹åˆ°å›¾ç‰‡: {item_name}")
            
            if is_image:
                try:
                    content = item.get_content()
                    item_name_safe = item.get_name() if item.get_name() else f"image_{len(image_mapping)}"
                    
                    # æå–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
                    filename = item_name_safe.split('/')[-1].split('\\\\')[-1]
                    
                    # ä¿å­˜å›¾ç‰‡
                    image_path = images_dir / filename
                    with open(image_path, 'wb') as f:
                        f.write(content)
                    
                    # å»ºç«‹æ˜ å°„ï¼šæ–‡ä»¶å -> URL
                    new_url = f"/api/reading/image/{epub_hash}/{filename}"
                    image_mapping[filename] = new_url
                    
                    print(f"âœ“ æå–å›¾ç‰‡: {filename} -> {new_url}")
                except Exception as e:
                    print(f"âœ— æå–å›¾ç‰‡å¤±è´¥ {item_name_safe}: {str(e)}")
                    pass
        
        # ç„¶åæå–æ–‡æ¡£å†…å®¹
        for item in all_items:
            item_type = item.get_type()
            is_document = False
            
            if hasattr(ebooklib_epub, 'ITEM_DOCUMENT'):
                is_document = item_type == ebooklib_epub.ITEM_DOCUMENT
            else:
                if isinstance(item_type, tuple):
                    is_document = item_type[0] == 9  # DOCUMENT = (9, 'DOCUMENT')
                elif isinstance(item_type, int):
                    is_document = item_type == 9
                else:
                    try:
                        item.get_content()
                        is_document = True
                    except:
                        is_document = False
            
            if is_document:
                try:
                    content = item.get_content()
                    html_text = content.decode('utf-8', errors='ignore')
                    
                    # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
                    html_text = re.sub(r'<head[^>]*>.*?</head>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
                    html_text = re.sub(r'<script[^>]*>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
                    html_text = re.sub(r'<style[^>]*>.*?</style>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
                    html_text = re.sub(r'<meta[^>]*>', '', html_text, flags=re.IGNORECASE)
                    html_text = re.sub(r'<title[^>]*>.*?</title>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
                    
                    # æ›¿æ¢å›¾ç‰‡è·¯å¾„ - å¤„ç†srcä¸­çš„ä»»ä½•å¼•ç”¨å›¾ç‰‡çš„è·¯å¾„
                    def replace_src(match):
                        src = match.group(1)
                        # è·å–æ–‡ä»¶å
                        img_filename = src.split('/')[-1].split('\\\\')[-1]
                        print(f"  å¤„ç†å›¾ç‰‡src: {src} -> æ–‡ä»¶å: {img_filename}, æ˜ å°„è¡¨å¤§å°: {len(image_mapping)}")
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„ä¸­
                        if img_filename in image_mapping:
                            new_url = image_mapping[img_filename]
                            print(f"  âœ“ æ˜ å°„åˆ°: {new_url}")
                            return f'src="{new_url}"'
                        else:
                            print(f"  âœ— æœªæ‰¾åˆ°æ˜ å°„")
                        return match.group(0)  # ä¿ç•™åŸæ ·
                    
                    # æ›¿æ¢æ‰€æœ‰srcå±æ€§ï¼ˆå¤šç§æ ¼å¼ï¼‰
                    # æ ¼å¼1: src="..." æˆ– src='...'
                    html_text = re.sub(r'src\s*=\s*["\']([^"\']*)["\']', replace_src, html_text, flags=re.IGNORECASE)
                    # æ ¼å¼2: src=... (æ²¡æœ‰å¼•å·)
                    html_text = re.sub(r'src\s*=\s*([^\s>]+)', replace_src, html_text, flags=re.IGNORECASE)
                    
                    html_text = html_text.strip()
                    if html_text:
                        text_content.append(html_text)
                except Exception as e:
                    print(f"âœ— æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
                    pass
    except Exception as e:
        raise Exception(f"EPUBæå–é”™è¯¯: {str(e)}")
    
    print(f"âœ“ EPUBæå–å®Œæˆï¼Œæ‰¾åˆ° {len(image_mapping)} å¼ å›¾ç‰‡ï¼Œ{len(text_content)} ä¸ªæ–‡æ¡£")
    return '\n'.join(text_content)


def extract_text_from_txt(file_path: str) -> str:
    """ä»TXTæ–‡ä»¶è¯»å–æ–‡æœ¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        with open(file_path, 'r', encoding='gbk') as file:
            return file.read()
    except Exception as e:
        raise Exception(f"TXTè¯»å–é”™è¯¯: {str(e)}")


def extract_text_from_doc(file_path: str) -> str:
    """ä»DOC/DOCXæå–æ–‡æœ¬ï¼Œå°½é‡æŒ‰é¡µå±•ç¤ºï¼šé‡åˆ°åˆ†é¡µç¬¦æ—¶æ¢é¡µï¼Œå¦åˆ™æŒ‰æ®µè½ç»„åˆ"""
    try:
        from docx import Document
        try:
            from docx.enum.text import WD_BREAK
        except Exception:
            WD_BREAK = None
    except ImportError:
        raise ImportError("python-docx not installed. Please install it: pip install python-docx")
    
    try:
        doc = Document(file_path)
        pages: list[list[str]] = [[]]
        for para in doc.paragraphs:
            text = para.text or ""
            # æ£€æµ‹æ®µå†…æ˜¯å¦å«åˆ†é¡µç¬¦
            has_page_break = False
            if WD_BREAK:
                for run in para.runs:
                    if getattr(run, "break_type", None) == WD_BREAK.PAGE:
                        has_page_break = True
                        break
            # å…ˆè®°å½•å½“å‰æ®µæ–‡æœ¬
            if text.strip():
                pages[-1].append(text)
            # å¦‚æœ‰åˆ†é¡µç¬¦ï¼Œå¼€å¯æ–°é¡µ
            if has_page_break:
                pages.append([])
        # æ”¶å°¾ç©ºé¡µæ¸…ç†
        pages = [p for p in pages if any(seg.strip() for seg in p)] or [[]]

        pages_html = []
        total_pages = len(pages)
        for idx, page in enumerate(pages, 1):
            escaped = html.escape('\n'.join(page))
            page_html = (
                f'<div class="doc-page">'
                f'<div class="page-number">ç¬¬ {idx} é¡µ / {total_pages}</div>'
                f'<pre>{escaped}</pre>'
                f'</div>'
            )
            pages_html.append(page_html)
        return '\n'.join(pages_html)
    except Exception as e:
        raise Exception(f"Wordæ–‡æ¡£æå–é”™è¯¯: {str(e)}")


def paginate_text(text: str, chars_per_page: int = 1500) -> List[str]:
    """å°†æ–‡æœ¬åˆ†é¡µ"""
    pages = []
    current_page = ""
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n')
    
    for para in paragraphs:
        if len(current_page) + len(para) + 1 > chars_per_page:
            if current_page:
                pages.append(current_page)
            current_page = para
        else:
            if current_page:
                current_page += '\n' + para
            else:
                current_page = para
    
    if current_page:
        pages.append(current_page)
    
    return pages if pages else [""]


def extract_words_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å•è¯ï¼ˆä¿„è¯­ï¼‰"""
    # ä¿„è¯­å•è¯æ¨¡å¼ï¼šå­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ã€æ’‡å·
    pattern = r"[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ\w'-]+"
    words = re.findall(pattern, text.lower())
    return list(set(words))  # å»é‡


def count_total_words(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬ä¸­çš„æ€»è¯æ•°ï¼ˆæ‰€æœ‰è¯æ±‡ï¼ŒåŒ…æ‹¬é‡å¤ï¼‰
    
    è§„åˆ™ï¼š
    - ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦è®¡æ•°ï¼ˆæ±‰å­—ï¼‰
    - ä¿„æ–‡å’Œå…¶ä»–ï¼šæŒ‰ç©ºæ ¼åˆ†è¯
    """
    if not text:
        return 0
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦ï¼ˆCJKï¼‰
    cjk_pattern = r"[\u4e00-\u9fff\u3400-\u4dbf]"  # æ±‰å­—èŒƒå›´
    cjk_chars = re.findall(cjk_pattern, text)
    cjk_count = len(cjk_chars)
    
    # å»é™¤ä¸­æ–‡å­—ç¬¦åçš„æ–‡æœ¬
    text_without_cjk = re.sub(cjk_pattern, " ", text)
    
    # ç»Ÿè®¡å…¶ä»–è¯­è¨€çš„è¯æ±‡ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
    other_pattern = r"[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ\w'-]+"
    other_words = re.findall(other_pattern, text_without_cjk)
    other_count = len(other_words)
    
    return cjk_count + other_count


# --- Scoring helpers -------------------------------------------------------

def sequence_similarity(reference: str, hypothesis: str) -> Dict[str, Any]:
    from difflib import SequenceMatcher

    matcher = SequenceMatcher(None, reference.lower(), hypothesis.lower())
    similarity = round(matcher.ratio() * 100, 2)

    ref_tokens = reference.split()
    hyp_tokens = hypothesis.split()
    token_mismatches: List[Dict[str, Any]] = []
    max_len = max(len(ref_tokens), len(hyp_tokens))
    for i in range(max_len):
        ref_tok = ref_tokens[i] if i < len(ref_tokens) else None
        hyp_tok = hyp_tokens[i] if i < len(hyp_tokens) else None
        if ref_tok != hyp_tok:
            token_mismatches.append({"index": i, "reference": ref_tok, "hypothesis": hyp_tok})

    return {"similarity": similarity, "mismatches": token_mismatches}


# --- Routes ----------------------------------------------------------------

@app.route("/api/health", methods=["GET"])
def health():
    return jsonify({
        "status": "ok",
        "whisper": whisper is not None,
        "ffmpeg": check_ffmpeg(),
        "gpu": get_device() == "cuda",
        "device": get_device()
    })


@app.route("/api/user-data/size", methods=["GET"])
def get_user_data_size():
    """è·å–ç”¨æˆ·æ•°æ®æ–‡ä»¶å¤¹çš„è¯¦ç»†å¤§å°ç»Ÿè®¡"""
    try:
        stats = {
            "media": 0,      # åª’ä½“æ–‡ä»¶
            "subtitles": 0,  # å­—å¹•æ–‡ä»¶
            "vocab": 0,      # ç”Ÿè¯æœ¬æ•°æ®
            "playlists": 0,  # æ’­æ”¾åˆ—è¡¨æ•°æ®
            "settings": 0,   # è®¾ç½®æ•°æ®
            "total": 0
        }
        
        print(f"[ç»Ÿè®¡æ•°æ®] user_data è·¯å¾„: {USER_DATA_DIR}")
        print(f"[ç»Ÿè®¡æ•°æ®] user_data å­˜åœ¨: {USER_DATA_DIR.exists()}")
        
        if USER_DATA_DIR.exists():
            for subdir, size_key in [
                ("media", "media"),
                ("subtitles", "subtitles"),
                ("vocab", "vocab"),
                ("playlists", "playlists"),
                ("settings", "settings")
            ]:
                subdir_path = USER_DATA_DIR / subdir
                if subdir_path.exists():
                    for item in subdir_path.rglob("*"):
                        if item.is_file():
                            file_size = item.stat().st_size
                            stats[size_key] += file_size
                            print(f"[ç»Ÿè®¡æ•°æ®] {subdir}/{item.name}: {file_size} bytes")
            
            # è®¡ç®—æ€»å¤§å°
            stats["total"] = sum(v for k, v in stats.items() if k != "total")
        
        print(f"[ç»Ÿè®¡æ•°æ®] æœ€ç»ˆç»Ÿè®¡: {stats}")
        
        # è½¬æ¢ä¸ºæ›´å‹å¥½çš„æ ¼å¼ï¼ˆbytesï¼‰
        return jsonify({
            "status": "success",
            "bytes": stats,
            "total_bytes": stats["total"],
            "total_kb": round(stats["total"] / 1024, 2)
        })
    except Exception as e:
        print(f"[ç»Ÿè®¡æ•°æ®] é”™è¯¯: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500


@app.route("/api/transcribe/progress", methods=["GET"])
def get_transcribe_progress():
    """è·å–å®æ—¶è½¬å½•è¿›åº¦"""
    global _transcribe_progress
    return jsonify(_transcribe_progress)


@app.route("/api/transcribe", methods=["POST"])
def transcribe():
    global _transcribe_progress
    if "audio" not in request.files:
        return jsonify({"error": "audio file missing"}), 400
    language = request.form.get("language")
    audio_file = request.files["audio"]

    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(audio_file.filename).suffix or ".wav") as tmp:
        audio_file.save(tmp.name)
        tmp_path = Path(tmp.name)

    try:
        _transcribe_progress = {"status": "å¼€å§‹è½¬å½•...", "progress": 0}
        result = run_whisper_transcribe(tmp_path, language)
        _transcribe_progress = {"status": "å®Œæˆ", "progress": 100}
        return jsonify({**result, "status": "success"})
    except Exception as e:
        _transcribe_progress = {"status": "é”™è¯¯", "progress": 0, "error": str(e)}
        return jsonify({"status": "error", "error": str(e)}), 500
    finally:
        tmp_path.unlink(missing_ok=True)


@app.route("/api/score", methods=["POST"])
def score():
    payload = request.get_json(force=True)
    reference = payload.get("reference", "")
    hypothesis = payload.get("hypothesis", "")
    if not reference:
        return jsonify({"error": "reference text required"}), 400
    metrics = sequence_similarity(reference, hypothesis)
    return jsonify(metrics)


@app.route("/api/subtitles/generate", methods=["POST"])
def generate_subtitles():
    global _transcribe_progress
    
    # æ”¯æŒä¸¤ç§æ–¹å¼ï¼šä¸Šä¼ æ–‡ä»¶ æˆ– ä½¿ç”¨å·²æœ‰çš„æ–‡ä»¶å
    audio_file = None
    tmp_path = None
    base_name = None
    
    if "audio" in request.files:
        # æ–¹å¼1ï¼šç›´æ¥ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        audio_file = request.files["audio"]
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(audio_file.filename).suffix or ".wav") as tmp:
            audio_file.save(tmp.name)
            tmp_path = Path(tmp.name)
        base_name = Path(audio_file.filename).stem
    elif "filename" in request.form:
        # æ–¹å¼2ï¼šä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶åï¼ˆä»æ’­æ”¾åˆ—è¡¨ï¼‰
        filename = request.form.get("filename")
        
        # å°è¯•å¤šä¸ªä½ç½®æŸ¥æ‰¾æ–‡ä»¶ï¼ˆæ’­æ”¾åˆ—è¡¨ä¸­çš„æ–‡ä»¶åœ¨ user_data/mediaï¼‰
        possible_paths = [
            USER_DATA_DIR / "media" / filename,  # æ’­æ”¾åˆ—è¡¨ä¸­çš„åª’ä½“æ–‡ä»¶
            USER_DATA_DIR / filename,             # user_data æ ¹ç›®å½•
            Path("user_data") / "media" / filename,
            Path("user_data") / filename,
            Path("") / filename,  # å½“å‰ç›®å½•
        ]
        
        tmp_path = None
        for path in possible_paths:
            if path.exists():
                tmp_path = path
                break
        
        if tmp_path is None:
            return jsonify({"error": f"file not found: {filename}", "status": "error"}), 400
        
        base_name = Path(filename).stem
    else:
        return jsonify({"error": "audio file or filename missing", "status": "error"}), 400
    
    language = request.form.get("language")

    try:
        _transcribe_progress = {"status": "å¼€å§‹ç”Ÿæˆå­—å¹•...", "progress": 0}
        result = run_whisper_transcribe(tmp_path, language)
        subtitles = [
            {
                "start": seg.get("start", 0.0),
                "end": seg.get("end", 0.0),
                "en": seg.get("text", ""),
                "zh": "",
                "userEn": "",
                "userZh": "",
                "note": "",
            }
            for seg in result.get("segments", [])
        ]
        
        # ä¿å­˜ç”Ÿæˆçš„å­—å¹•åˆ°åª’ä½“æ–‡ä»¶æ‰€åœ¨ç›®å½•
        media_path = tmp_path.parent
        subtitle_path = media_path / f"{base_name}.json"
        subtitle_path.parent.mkdir(parents=True, exist_ok=True)
        with open(subtitle_path, "w", encoding="utf-8") as f:
            json.dump(subtitles, f, ensure_ascii=False, indent=2)
        print(f"âœ“ å­—å¹•å·²ä¿å­˜: {subtitle_path}")
        
        _transcribe_progress = {"status": "å®Œæˆ", "progress": 100}
        return jsonify({"subtitles": subtitles, "raw": result.get("text", ""), "status": "success"})
    except Exception as e:
        _transcribe_progress = {"status": "é”™è¯¯", "progress": 0, "error": str(e)}
        return jsonify({"status": "error", "error": str(e), "subtitles": [], "raw": ""}), 500
    finally:
        # åªåˆ é™¤é€šè¿‡ä¸Šä¼ åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶
        if audio_file is not None and tmp_path is not None:
            tmp_path.unlink(missing_ok=True)


@app.route("/api/subtitles/save", methods=["POST"])
def save_subtitles():
    """ä¿å­˜ç”¨æˆ·ç¼–è¾‘çš„å­—å¹•"""
    try:
        payload = request.get_json(force=True)
        media_name = payload.get("mediaName", "untitled")
        subtitles = payload.get("subtitles", [])
        
        base_name = Path(media_name).stem
        
        # å°è¯•åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾åª’ä½“æ–‡ä»¶
        possible_paths = [
            USER_DATA_DIR / "media" / media_name,
            USER_DATA_DIR / media_name,
            Path("user_data") / "media" / media_name,
            Path("user_data") / media_name,
        ]
        
        media_path = None
        for path in possible_paths:
            if path.exists():
                media_path = path.parent
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°åª’ä½“æ–‡ä»¶ï¼Œé»˜è®¤ä¿å­˜åˆ° media ç›®å½•
        if media_path is None:
            media_path = get_user_file_path("", "media")
        
        subtitle_path = media_path / f"{base_name}.json"
        subtitle_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(subtitle_path, "w", encoding="utf-8") as f:
            json.dump(subtitles, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(subtitle_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/subtitles/scan", methods=["GET"])
def scan_subtitle_files():
    """æ‰«æåª’ä½“æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„å­—å¹•æ–‡ä»¶"""
    try:
        media_name = request.args.get("media", "")
        if not media_name:
            return jsonify({"status": "error", "error": "media parameter required"}), 400
        
        from urllib.parse import unquote
        media_name = unquote(media_name)
        media_base = Path(media_name).stem
        
        # å°è¯•åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾åª’ä½“æ–‡ä»¶
        possible_paths = [
            USER_DATA_DIR / "media" / media_name,
            USER_DATA_DIR / media_name,
            Path("user_data") / "media" / media_name,
            Path("user_data") / media_name,
        ]
        
        media_path = None
        for path in possible_paths:
            if path.exists():
                media_path = path.parent
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°åª’ä½“æ–‡ä»¶ï¼Œä½¿ç”¨ media ç›®å½•
        if media_path is None:
            media_path = get_user_file_path("", "media")
        
        if not media_path.exists():
            return jsonify({"status": "not_found", "files": []})
        
        found_files = []
        
        for item in media_path.iterdir():
            if item.is_file() and item.suffix.lower() in ['.srt', '.vtt', '.ass', '.ssa', '.json']:
                file_base = item.stem
                if file_base == media_base:
                    found_files.append({
                        "filename": item.name,
                        "format": item.suffix.lower()[1:],
                        "size": item.stat().st_size
                    })
        
        print(f"[DEBUG] scan_subtitle_files: media_name={media_name}, media_base={media_base}, media_path={media_path}, found_files={found_files}")
        return jsonify({"status": "success", "files": found_files})
    except Exception as e:
        print(f"[DEBUG] scan_subtitle_files error: {e}")
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/subtitles/load-file/<path:filename>", methods=["GET"])
def load_subtitle_file(filename):
    """åŠ è½½å¹¶è§£æåª’ä½“æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„å­—å¹•æ–‡ä»¶"""
    try:
        from urllib.parse import unquote
        filename = unquote(filename)
        
        # å°è¯•åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾åª’ä½“æ–‡ä»¶
        possible_paths = [
            USER_DATA_DIR / "media" / filename,
            USER_DATA_DIR / filename,
            Path("user_data") / "media" / filename,
            Path("user_data") / filename,
        ]
        
        subtitle_path = None
        for path in possible_paths:
            if path.exists():
                subtitle_path = path
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨ subtitles ç›®å½•æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼‰
        if subtitle_path is None or not subtitle_path.exists():
            subtitle_path = get_user_file_path(filename, "subtitles")
        
        if not subtitle_path.exists():
            return jsonify({"status": "not_found", "subtitles": []}), 404
        
        suffix = subtitle_path.suffix.lower()
        
        subtitles = []
        if suffix == '.json':
            with open(subtitle_path, "r", encoding="utf-8") as f:
                subtitles = json.load(f)
        elif suffix == '.srt':
            with open(subtitle_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
            subtitles = parse_srt(content)
        elif suffix == '.vtt':
            with open(subtitle_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
            subtitles = parse_vtt(content)
        elif suffix in ['.ass', '.ssa']:
            return jsonify({"status": "error", "error": "ASS/SSA format not supported yet"}), 400
        
        return jsonify({"status": "success", "subtitles": subtitles})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/subtitles/load/<path:filename>", methods=["GET"])
def load_subtitles(filename):
    """åŠ è½½ä¿å­˜çš„å­—å¹•æ–‡ä»¶"""
    try:
        from urllib.parse import unquote
        filename = unquote(filename)
        base_name = Path(filename).stem
        
        # å°è¯•åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾åª’ä½“æ–‡ä»¶å’Œå­—å¹•æ–‡ä»¶
        possible_paths = [
            USER_DATA_DIR / "media" / filename,
            USER_DATA_DIR / filename,
            Path("user_data") / "media" / filename,
            Path("user_data") / filename,
        ]
        
        subtitle_path = None
        for path in possible_paths:
            # æŸ¥æ‰¾åŒç›®å½•ä¸‹çš„å­—å¹•æ–‡ä»¶
            subtitle_path = path.parent / f"{base_name}.json"
            if subtitle_path.exists():
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨ subtitles ç›®å½•æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼‰
        if subtitle_path is None or not subtitle_path.exists():
            subtitle_path = get_user_file_path(f"{base_name}.json", "subtitles")
        
        if subtitle_path.exists():
            with open(subtitle_path, "r", encoding="utf-8") as f:
                subtitles = json.load(f)
            return jsonify({"status": "success", "subtitles": subtitles})
        else:
            return jsonify({"status": "not_found", "subtitles": []}), 404
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


def parse_srt(content: str) -> List[Dict[str, Any]]:
    """è§£æ SRT æ ¼å¼å­—å¹•"""
    subtitles = []
    pattern = re.compile(r'(\d+)\s*\n(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*\n(.*?)(?=\n\n|\n\d+\s*\n|\Z)', re.DOTALL)
    
    for match in pattern.finditer(content):
        start = int(match.group(2)) * 3600 + int(match.group(3)) * 60 + int(match.group(4)) + int(match.group(5)) / 1000
        end = int(match.group(6)) * 3600 + int(match.group(7)) * 60 + int(match.group(8)) + int(match.group(9)) / 1000
        text = match.group(10).strip().replace('\n', ' ')
        
        subtitles.append({
            "start": start,
            "end": end,
            "text": text
        })
    
    return subtitles


def parse_vtt(content: str) -> List[Dict[str, Any]]:
    """è§£æ VTT æ ¼å¼å­—å¹•"""
    subtitles = []
    lines = content.split('\n')
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        if '-->' in line:
            time_match = re.match(r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})\.(\d{3})', line)
            if not time_match:
                time_match = re.match(r'(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2})\.(\d{3})', line)
            
            if time_match:
                groups = time_match.groups()
                if len(groups) == 8:
                    start = int(groups[0]) * 3600 + int(groups[1]) * 60 + int(groups[2]) + int(groups[3]) / 1000
                    end = int(groups[4]) * 3600 + int(groups[5]) * 60 + int(groups[6]) + int(groups[7]) / 1000
                else:
                    start = int(groups[0]) * 60 + int(groups[1]) + int(groups[2]) / 1000
                    end = int(groups[3]) * 60 + int(groups[4]) + int(groups[5]) / 1000
                
                i += 1
                text_lines = []
                while i < len(lines) and lines[i].strip() and not '-->' in lines[i]:
                    text_lines.append(lines[i].strip())
                    i += 1
                
                text = ' '.join(text_lines)
                if text:
                    subtitles.append({
                        "start": start,
                        "end": end,
                        "text": text
                    })
            else:
                i += 1
        else:
            i += 1
    
    return subtitles


@app.route("/api/media/upload", methods=["POST"])
def upload_media():
    """ä¸Šä¼ åª’ä½“æ–‡ä»¶åˆ°æœåŠ¡å™¨"""
    try:
        if "media" not in request.files:
            return jsonify({"error": "media file missing", "status": "error"}), 400
        
        media_file = request.files["media"]
        if not media_file.filename:
            return jsonify({"error": "no filename", "status": "error"}), 400
        
        # è·å–è·¯å¾„å‚æ•°
        path = request.form.get("path", "")
        
        # æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„
        if path:
            # ç¡®ä¿è·¯å¾„ä»¥ '/' ç»“å°¾
            if not path.endswith('/'):
                path += '/'
            # ä¿å­˜åˆ° user_data/media/[path] æ–‡ä»¶å¤¹
            media_path = get_user_file_path(path + media_file.filename, "media")
        else:
            # ä¿å­˜åˆ° user_data/media æ ¹æ–‡ä»¶å¤¹
            media_path = get_user_file_path(media_file.filename, "media")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        media_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        media_file.save(str(media_path))
        
        # æ„å»ºè¿”å›çš„æ–‡ä»¶åï¼ˆåŒ…å«è·¯å¾„ï¼‰
        if path:
            return_filename = path + media_file.filename
        else:
            return_filename = media_file.filename
        
        return jsonify({
            "status": "success",
            "path": str(media_path),
            "filename": return_filename
        })
    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ä»¥ä¾¿å‰ç«¯æç¤º
        print(f"[upload_media] error: {e}")
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/media/load/<path:filename>", methods=["GET"])
def load_media(filename):
    """ä»æœåŠ¡å™¨åŠ è½½åª’ä½“æ–‡ä»¶"""
    try:
        from urllib.parse import unquote
        filename = unquote(filename)
        media_path = get_user_file_path(filename, "media")
        
        if media_path.exists():
            return send_from_directory(
                media_path.parent,
                media_path.name,
                as_attachment=False
            )
        else:
            return jsonify({"status": "not_found"}), 404
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/media/scan", methods=["GET"])
def scan_media_files():
    """æ‰«ææœ¬åœ°åª’ä½“æ–‡ä»¶å’Œæ–‡ä»¶å¤¹çš„å˜åŒ–"""
    print("[DEBUG] scan_media_files endpoint called")
    try:
        media_dir = get_user_file_path("", "media")
        print(f"[DEBUG] Media directory: {media_dir}")
        print(f"[DEBUG] Media directory exists: {media_dir.exists()}")
        if not media_dir.exists():
            return jsonify({"status": "success", "files": [], "folders": []})
        
        files = []
        folders = []
        
        # é€’å½’æ‰«æç›®å½•
        def scan_directory(path, relative_path=""):
            for item in path.iterdir():
                if item.is_dir():
                    folder_path = relative_path + item.name + "/"
                    folders.append(folder_path)
                    scan_directory(item, folder_path)
                else:
                    file_path = relative_path + item.name
                    files.append({
                        "path": file_path,
                        "size": item.stat().st_size,
                        "mtime": item.stat().st_mtime
                    })
        
        scan_directory(media_dir)
        print(f"[DEBUG] Found {len(files)} files and {len(folders)} folders")
        
        return jsonify({
            "status": "success",
            "files": files,
            "folders": folders
        })
    except Exception as e:
        print(f"[DEBUG] Error in scan_media_files: {e}")
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/media/clear", methods=["POST"])
def clear_media():
    """æ¸…ç©ºåª’ä½“æ–‡ä»¶å¤¹"""
    try:
        media_dir = get_user_file_path("", "media")
        
        if not media_dir.exists():
            return jsonify({"status": "success"})
        
        # é€’å½’åˆ é™¤æ‰€æœ‰å†…å®¹
        import shutil
        for item in media_dir.iterdir():
            if item.is_dir():
                shutil.rmtree(str(item))
            else:
                item.unlink()
        
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/media/delete", methods=["POST"])
def delete_media():
    """åˆ é™¤æŒ‡å®šçš„åª’ä½“æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json()
        filename = payload.get("filename")
        
        if not filename:
            return jsonify({"status": "error", "error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"}), 400
        
        media_dir = get_user_file_path("", "media")
        file_path = media_dir / filename
        
        if not file_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
        
        import shutil
        if file_path.is_dir():
            shutil.rmtree(str(file_path))
        else:
            file_path.unlink()
        
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/vocab/save", methods=["POST"])
def save_vocab():
    """ä¿å­˜ç”Ÿè¯æœ¬æ•°æ®åˆ°æ–‡ä»¶ï¼ˆæ—§ç‰ˆAPIï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
    try:
        payload = request.get_json(force=True)
        vocab = payload.get("vocab", [])
        
        vocab_path = get_user_file_path("vocab.json", "vocab")
        
        with open(vocab_path, "w", encoding="utf-8") as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(vocab_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/vocab/load", methods=["GET"])
def load_vocab():
    """åŠ è½½ä¿å­˜çš„ç”Ÿè¯æœ¬æ•°æ®ï¼ˆæ—§ç‰ˆAPIï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
    try:
        vocab_path = get_user_file_path("vocab.json", "vocab")
        
        if vocab_path.exists():
            with open(vocab_path, "r", encoding="utf-8") as f:
                vocab = json.load(f)
            return jsonify({"status": "success", "vocab": vocab})
        else:
            return jsonify({"status": "success", "vocab": []})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e), "vocab": []}), 500


@app.route("/api/vocabbooks/save", methods=["POST"])
def save_vocabbooks():
    """ä¿å­˜å¤šä¸ªç”Ÿè¯æœ¬æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        payload = request.get_json(force=True)
        vocabbooks = payload.get("vocabBooks", [])
        current_id = payload.get("currentVocabBookId", None)
        
        # ä¿å­˜ç”Ÿè¯æœ¬æ•°æ®
        vocabbooks_path = get_user_file_path("vocabbooks.json", "vocab")
        with open(vocabbooks_path, "w", encoding="utf-8") as f:
            json.dump({
                "vocabBooks": vocabbooks,
                "currentVocabBookId": current_id
            }, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(vocabbooks_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/vocabbooks/load", methods=["GET"])
def load_vocabbooks():
    """åŠ è½½ä¿å­˜çš„å¤šä¸ªç”Ÿè¯æœ¬æ•°æ®"""
    try:
        vocabbooks_path = get_user_file_path("vocabbooks.json", "vocab")
        
        if vocabbooks_path.exists():
            with open(vocabbooks_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return jsonify({
                "status": "success",
                "vocabBooks": data.get("vocabBooks", []),
                "currentVocabBookId": data.get("currentVocabBookId", None)
            })
        else:
            return jsonify({
                "status": "success",
                "vocabBooks": [],
                "currentVocabBookId": None
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e), "vocabBooks": [], "currentVocabBookId": None}), 500





@app.route("/api/settings/save", methods=["POST"])
def save_settings():
    """ä¿å­˜ç”¨æˆ·è®¾ç½®åˆ°æ–‡ä»¶"""
    try:
        payload = request.get_json(force=True)
        settings = payload.get("settings", {})
        
        settings_path = get_user_file_path("settings.json", "settings")
        with open(settings_path, "w", encoding="utf-8") as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(settings_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/settings/load", methods=["GET"])
def load_settings():
    """åŠ è½½ä¿å­˜çš„ç”¨æˆ·è®¾ç½®"""
    try:
        settings_path = get_user_file_path("settings.json", "settings")
        
        if settings_path.exists():
            with open(settings_path, "r", encoding="utf-8") as f:
                settings = json.load(f)
            return jsonify({"status": "success", "settings": settings})
        else:
            return jsonify({"status": "success", "settings": {}})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e), "settings": {}}), 500


@app.route("/api/playlist/save", methods=["POST"])
def save_playlist():
    """ä¿å­˜æ’­æ”¾åˆ—è¡¨åˆ°æ–‡ä»¶"""
    try:
        payload = request.get_json(force=True)
        playlist = payload.get("playlist", [])
        current_index = payload.get("currentPlaylistIndex", -1)
        
        playlist_data = {
            "playlist": playlist,
            "currentPlaylistIndex": current_index
        }
        
        playlist_path = get_user_file_path("playlist.json", "settings")
        with open(playlist_path, "w", encoding="utf-8") as f:
            json.dump(playlist_data, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(playlist_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/playlist/load", methods=["GET"])
def load_playlist():
    """åŠ è½½ä¿å­˜çš„æ’­æ”¾åˆ—è¡¨"""
    try:
        playlist_path = get_user_file_path("playlist.json", "settings")
        
        if playlist_path.exists():
            with open(playlist_path, "r", encoding="utf-8") as f:
                playlist_data = json.load(f)
            return jsonify({
                "status": "success",
                "playlist": playlist_data.get("playlist", []),
                "currentPlaylistIndex": playlist_data.get("currentPlaylistIndex", -1)
            })
        else:
            return jsonify({
                "status": "success",
                "playlist": [],
                "currentPlaylistIndex": -1
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e), "playlist": [], "currentPlaylistIndex": -1}), 500


@app.route("/api/playlist/create-folder", methods=["POST"])
def create_playlist_folder():
    """åˆ›å»ºæ’­æ”¾åˆ—è¡¨æ–‡ä»¶å¤¹"""
    try:
        import json
        
        # ç›´æ¥ä½¿ç”¨ request.get_json()ï¼ŒFlask ä¼šè‡ªåŠ¨å¤„ç†ç¼–ç 
        payload = request.get_json()
        folder_name = payload.get("folder_name")
        
        print(f"[create_folder] æ”¶åˆ°çš„æ–‡ä»¶å¤¹åç§°: {folder_name}, ç±»å‹: {type(folder_name)}")
        print(f"[create_folder] æ–‡ä»¶å¤¹åç§°ç¼–ç : {folder_name.encode('utf-8') if folder_name else None}")
        
        if not folder_name:
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º"}), 400
        
        import os
        base_dir = get_user_file_path("", "media")
        folder_path = os.path.join(str(base_dir), folder_name)
        
        print(f"[create_folder] å®Œæ•´è·¯å¾„: {folder_path}")
        print(f"[create_folder] è·¯å¾„ç¼–ç : {folder_path.encode('utf-8')}")
        
        if os.path.exists(folder_path):
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹å·²å­˜åœ¨"}), 400
        
        os.makedirs(folder_path, exist_ok=True)
        
        print(f"[create_folder] æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {folder_name}")
        
        return jsonify({
            "status": "success",
            "folder_path": folder_name
        })
    except Exception as e:
        print(f"[create_folder] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/playlist/move-item", methods=["POST"])
def move_playlist_item():
    """ç§»åŠ¨æ’­æ”¾åˆ—è¡¨é¡¹åˆ°æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        source_name = payload.get("source_name")
        target_folder = payload.get("target_folder")
        
        if not source_name:
            return jsonify({"status": "error", "error": "æºæ–‡ä»¶åä¸èƒ½ä¸ºç©º"}), 400
        
        if not target_folder:
            return jsonify({"status": "error", "error": "ç›®æ ‡æ–‡ä»¶å¤¹ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "media")
        source_path = base_dir / source_name
        target_path = base_dir / target_folder / source_name.split("/")[-1]
        
        if not source_path.exists():
            return jsonify({"status": "error", "error": "æºæ–‡ä»¶ä¸å­˜åœ¨"}), 404
        
        if not target_path.parent.exists():
            return jsonify({"status": "error", "error": "ç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 404
        
        # ç§»åŠ¨æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
        import shutil
        if source_path.is_dir():
            shutil.move(str(source_path), str(target_path))
        else:
            shutil.move(str(source_path), str(target_path))
        
        return jsonify({
            "status": "success",
            "source": source_name,
            "target": f"{target_folder}/{source_name.split('/')[-1]}"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/playlist/delete-folder", methods=["POST"])
def delete_playlist_folder():
    """åˆ é™¤æ’­æ”¾åˆ—è¡¨æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        folder_name = payload.get("folder_name")
        
        if not folder_name:
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "media")
        folder_path = base_dir / folder_name
        
        if not folder_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 404
        
        # é€’å½’åˆ é™¤æ–‡ä»¶å¤¹
        import shutil
        shutil.rmtree(str(folder_path))
        
        return jsonify({
            "status": "success",
            "folder_name": folder_name
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/playlist/rename-folder", methods=["POST"])
def rename_playlist_folder():
    """é‡å‘½åæ’­æ”¾åˆ—è¡¨æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        old_name = payload.get("old_name")
        new_name = payload.get("new_name")
        
        if not old_name:
            return jsonify({"status": "error", "error": "æ—§æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º"}), 400
        
        if not new_name:
            return jsonify({"status": "error", "error": "æ–°æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "media")
        old_path = base_dir / old_name
        new_path = base_dir / new_name
        
        if not old_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 404
        
        if new_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹å·²å­˜åœ¨"}), 400
        
        old_path.rename(new_path)
        
        return jsonify({
            "status": "success",
            "old_name": old_name,
            "new_name": new_name
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/playlist/scan", methods=["GET"])
def scan_playlist():
    """æ‰«ææ’­æ”¾åˆ—è¡¨æ–‡ä»¶å¤¹ç»“æ„"""
    try:
        media_dir = get_user_file_path("", "media")
        
        if not media_dir.exists():
            return jsonify({
                "status": "success",
                "playlist": []
            })
        
        playlist = []
        
        # é€’å½’æ‰«æç›®å½•
        def scan_directory(path, relative_path=""):
            for item in path.iterdir():
                item_name = item.name
                item_relative_path = relative_path + item_name
                
                if item.is_dir():
                    # æ·»åŠ æ–‡ä»¶å¤¹
                    playlist.append({
                        "name": item_relative_path + "/",
                        "type": "folder",
                        "url": None,
                        "serverPath": item_relative_path + "/"
                    })
                    # é€’å½’æ‰«æå­æ–‡ä»¶å¤¹
                    scan_directory(item, item_relative_path + "/")
                else:
                    # æ·»åŠ æ–‡ä»¶
                    playlist.append({
                        "name": item_relative_path,
                        "type": "file",
                        "url": None,
                        "serverPath": item_relative_path
                    })
        
        scan_directory(media_dir)
        
        return jsonify({
            "status": "success",
            "playlist": playlist
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e), "playlist": []}), 500


# --- Reading Module Routes --------------------------------------------------


@app.route("/api/reading/test-pdf", methods=["GET"])
def test_pdf_endpoint():
    """æµ‹è¯•endpoint - è¿”å›ç®€å•å“åº”"""
    print("[test_pdf_endpoint] è¢«è°ƒç”¨")
    return jsonify({"status": "ok", "message": "Test endpoint working"})

@app.route("/api/reading/raw/<path:doc_id>", methods=["GET", "HEAD"])
def serve_raw_document(doc_id):
    """è¿”å›åŸå§‹æ–‡ä»¶æˆ–è½¬æ¢åçš„PDFï¼Œä¾›å‰ç«¯åŸæ ·å±•ç¤º"""
    try:
        import time
        start_time = time.time()
        from urllib.parse import unquote, quote
        
        # URLè§£ç æ–‡ä»¶å
        doc_id = unquote(doc_id)
        print(f"[serve_raw_document] å¼€å§‹å¤„ç† doc_id: {doc_id}", flush=True)
        
        doc_index_path = get_user_file_path("documents.json", "readings")
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
        print(f"[serve_raw_document] å·²åŠ è½½documents.json, å…±{len(documents)}ä¸ªæ–‡æ¡£")

        # å°è¯•ç²¾ç¡®åŒ¹é…
        if doc_id not in documents:
            # å°è¯•ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
            doc_id_lower = doc_id.lower()
            for key in documents.keys():
                if key.lower() == doc_id_lower:
                    doc_id = key
                    break
            else:
                print(f"[serve_raw_document] âŒ æ–‡æ¡£ä¸å­˜åœ¨: {doc_id}")
                return jsonify({"status": "error", "error": f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_id}"}), 404

        meta = documents[doc_id]
        filename = meta.get("filename", doc_id)
        ext = meta.get("ext", "")  
        converted_pdf = meta.get("converted_pdf")
        print(f"[serve_raw_document] æ‰¾åˆ°æ–‡æ¡£: filename={filename}, ext={ext}")

        base_dir = get_user_file_path("", "readings")

        # ä¼˜å…ˆä½¿ç”¨è½¬æ¢åçš„PDFï¼ˆç”¨äºWordä¿æŒæ ·å¼ï¼‰
        if converted_pdf:
            pdf_path = Path(converted_pdf)
            if pdf_path.exists():
                print(f"[serve_raw_document] ä½¿ç”¨è½¬æ¢åçš„PDF: {pdf_path}")
                response = send_file(
                    pdf_path, 
                    mimetype="application/pdf",
                    as_attachment=False,
                    conditional=True  # å¯ç”¨æ¡ä»¶è¯·æ±‚ï¼ˆIf-Modified-Sinceç­‰ï¼‰
                )
                response.headers["Accept-Ranges"] = "bytes"
                # ç§»é™¤Content-Dispositionä»¥é¿å…è§¦å‘ä¸‹è½½
                response.headers.pop("Content-Disposition", None)
                return response

        # å¦åˆ™è¿”å›åŸæ–‡ä»¶
        folder = meta.get("folder", "")
        file_path = base_dir / folder / filename
        if not file_path.exists():
            # å°è¯•åœ¨æ ¹ç›®å½•æŸ¥æ‰¾ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            file_path = base_dir / filename
            if not file_path.exists():
                print(f"[serve_raw_document] âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return jsonify({"status": "error", "error": "æ–‡ä»¶å·²ä¸¢å¤±"}), 404

        file_size = file_path.stat().st_size
        print(f"[serve_raw_document] æ–‡ä»¶è·¯å¾„: {file_path}")
        print(f"[serve_raw_document] æ–‡ä»¶å¤§å°: {file_size} bytes ({file_size/1024/1024:.2f} MB)")

        guessed_type, _ = mimetypes.guess_type(file_path)
        # å¦‚æœæ— æ³•çŒœæµ‹MIMEç±»å‹ï¼Œæ ¹æ®æ‰©å±•åæ‰‹åŠ¨è®¾ç½®
        if not guessed_type:
            ext_lower = Path(file_path).suffix.lower()
            mime_map = {
                '.pdf': 'application/pdf',
                '.epub': 'application/epub+zip',
                '.txt': 'text/plain; charset=utf-8',
                '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                '.doc': 'application/msword'
            }
            guessed_type = mime_map.get(ext_lower, 'application/octet-stream')
        
        print(f"[serve_raw_document] MIME type: {guessed_type}")
        
        # å¯¹å¤§æ–‡ä»¶ä½¿ç”¨æµå¼ä¼ è¾“
        if file_size > 1024 * 1024:  # > 1MB
            print(f"[serve_raw_document] ä½¿ç”¨æµå¼ä¼ è¾“ï¼ˆæ–‡ä»¶>1MBï¼‰")
            
        response = send_file(
            file_path, 
            mimetype=guessed_type, 
            as_attachment=False,
            conditional=True,  # æ”¯æŒHTTPæ¡ä»¶è¯·æ±‚
            max_age=0  # ç¦ç”¨ç¼“å­˜
        )
        response.headers["Accept-Ranges"] = "bytes"
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        # ç§»é™¤Content-Dispositionä»¥é¿å…è§¦å‘ä¸‹è½½
        response.headers.pop("Content-Disposition", None)
        
        elapsed = time.time() - start_time
        print(f"[serve_raw_document] âœ… å®Œæˆ, è€—æ—¶ {elapsed:.2f}s")

        return response
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/upload-document", methods=["POST"])
def upload_document():
    """ä¸Šä¼ å¹¶è§£ææ–‡æ¡£ï¼ˆPDF, EPUB, TXT, DOCï¼‰"""
    try:
        if 'file' not in request.files:
            return jsonify({"status": "error", "error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({"status": "error", "error": "æ–‡ä»¶åä¸ºç©º"}), 400
        
        # è·å–ç›®æ ‡æ–‡ä»¶å¤¹ï¼ˆå¦‚æœæœ‰ï¼‰
        folder = request.form.get("folder", "")
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_dir = get_user_file_path("", "readings")
        temp_dir.mkdir(exist_ok=True)
        
        file_ext = Path(file.filename).suffix.lower()
        temp_path = temp_dir / f"temp_{os.urandom(8).hex()}{file_ext}"
        file.save(str(temp_path))
        
        print(f"ğŸ“„ Processing document: {file.filename}")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
        text = ""
        if file_ext == ".pdf":
            text = extract_text_from_pdf(str(temp_path))
        elif file_ext == ".epub":
            text = extract_text_from_epub(str(temp_path))
        elif file_ext in [".txt"]:
            text = extract_text_from_txt(str(temp_path))
        elif file_ext == ".md":
            # Markdown æ–‡ä»¶ç›´æ¥è¯»å–ä¸ºæ–‡æœ¬ï¼ˆå‰ç«¯ç”¨ marked.js è§£æï¼‰
            with open(temp_path, 'r', encoding='utf-8') as f:
                text = f.read()
        elif file_ext in [".doc", ".docx"]:
            text = extract_text_from_doc(str(temp_path))
        else:
            return jsonify({"status": "error", "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}), 400
        
        # ä¸å†è‡ªåŠ¨åˆ†é¡µï¼Œä¿å­˜å®Œæ•´æ–‡æœ¬
        # å¦‚æœéœ€è¦ï¼Œå‰ç«¯å¯ä»¥æ ¹æ®æ»šåŠ¨ä½ç½®è®¡ç®—é˜…è¯»è¿›åº¦
        
        # ç”Ÿæˆdoc_id
        doc_id = file.filename.replace(' ', '_').replace('.', '_')

        # ä¿å­˜æ–‡æ¡£åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        base_dir = get_user_file_path("", "readings")
        target_dir = base_dir / folder
        target_dir.mkdir(parents=True, exist_ok=True)
        
        # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹
        final_file_path = target_dir / file.filename
        temp_path.rename(final_file_path)

        # è‹¥ä¸º Wordï¼Œå°è¯•è½¬æ¢ä¸º PDF ä»¥ä¿ç•™æ ·å¼ï¼ˆéœ€ docx2pdfï¼Œå¯èƒ½åœ¨æ—  Office ç¯å¢ƒä¸‹å¤±è´¥ï¼‰
        converted_pdf_path = None
        if file_ext in [".doc", ".docx"]:
            try:
                from docx2pdf import convert  # type: ignore
                out_pdf = get_user_file_path(f"{doc_id}.pdf", "readings")
                convert(str(final_file_path), str(out_pdf))
                if out_pdf.exists():
                    converted_pdf_path = out_pdf
                    print(f"âœ“ Word è½¬ PDF æˆåŠŸ: {out_pdf}")
            except Exception as conv_err:
                print(f"âœ— Word è½¬ PDF å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æå–: {conv_err}")

        # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
        total_words = count_total_words(text)
        doc_metadata = {
            "filename": file.filename,
            "folder": folder,
            "size": len(text),
            "char_count": len(text),
            "total_words": total_words,
            "upload_time": str(final_file_path.stat().st_mtime),
            "ext": file_ext,
            "converted_pdf": str(converted_pdf_path) if converted_pdf_path else None
        }
        
        # ä¿å­˜åˆ°JSON
        doc_index_path = get_user_file_path("documents.json", "readings")
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
        
        documents[doc_id] = doc_metadata
        
        with open(doc_index_path, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å®Œæ•´æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºæ£€ç´¢/ç»Ÿè®¡ï¼›å±•ç¤ºä»ä½¿ç”¨åŸæ–‡ä»¶æˆ–è½¬æ¢åçš„PDFï¼‰
        content_path = get_user_file_path(f"{doc_id}_content.json", "readings")
        with open(content_path, 'w', encoding='utf-8') as f:
            json.dump({"text": text}, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            "status": "success",
            "doc_id": doc_id,
            "filename": file.filename,
            "folder": folder,
            "char_count": len(text),
            "total_words": total_words,
            "size": len(text),
            "sample": text[:500],  # è¿”å›å‰500å­—ä½œä¸ºé¢„è§ˆ
            "view_url": f"/api/reading/raw/{doc_id}"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/load-document/<doc_id>", methods=["GET"])
def load_document(doc_id):
    """åŠ è½½æŒ‡å®šæ–‡æ¡£çš„å†…å®¹"""
    try:
        content_path = get_user_file_path(f"{doc_id}_content.json", "readings")
        doc_index_path = get_user_file_path("documents.json", "readings")
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
        
        if not content_path.exists():
            return jsonify({"status": "error", "error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404
        
        with open(content_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        text = data.get("text", "")
        
        # è®¡ç®—æ€»è¯æ•°
        total_words = count_total_words(text)
        
        return jsonify({
            "status": "success",
            "text": text,
            "char_count": len(text),
            "total_words": total_words,
            "view_url": f"/api/reading/raw/{doc_id}",
            "metadata": documents.get(doc_id, {})
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/image/<epub_hash>/<filename>", methods=["GET"])
def serve_epub_image(epub_hash, filename):
    """æä¾›EPUBæå–çš„å›¾ç‰‡è®¿é—®"""
    try:
        print(f"DEBUG: æ”¶åˆ°å›¾ç‰‡è¯·æ±‚: epub_hash={epub_hash}, filename={filename}")
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        image_path = USER_DATA_DIR / "readings" / "reading_images" / epub_hash / filename
        print(f"DEBUG: å›¾ç‰‡è·¯å¾„: {image_path}")
        
        if not image_path.exists():
            print(f"DEBUG: å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return jsonify({"status": "error", "error": "å›¾ç‰‡ä¸å­˜åœ¨"}), 404
        
        # çŒœæµ‹MIMEç±»å‹
        guessed_type, _ = mimetypes.guess_type(str(image_path))
        if not guessed_type:
            guessed_type = "application/octet-stream"
        print(f"DEBUG: MIMEç±»å‹: {guessed_type}")
        
        return send_file(
            image_path,
            mimetype=guessed_type,
            as_attachment=False
        )
    except Exception as e:
        print(f"DEBUG: å›¾ç‰‡æœåŠ¡é”™è¯¯: {e}")
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/delete-document/<doc_id>", methods=["DELETE"])
def delete_document(doc_id):
    """åˆ é™¤æŒ‡å®šæ–‡æ¡£åŠå…¶ç›¸å…³æ–‡ä»¶ï¼ˆå†…å®¹ã€ç¬”è®°ã€åŸæ–‡ä»¶ã€æå–å›¾ç‰‡ã€è½¬æ¢åçš„PDFï¼‰"""
    try:
        import hashlib

        doc_index_path = get_user_file_path("documents.json", "readings")
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)

        if doc_id not in documents:
            return jsonify({"status": "error", "error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404

        filename = documents[doc_id].get("filename", doc_id)
        converted_pdf = documents[doc_id].get("converted_pdf")

        # ä»ç´¢å¼•ä¸­ç§»é™¤å¹¶ä¿å­˜
        documents.pop(doc_id, None)
        with open(doc_index_path, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)

        removed_files = []

        def remove_path(path: Path):
            if path.exists():
                try:
                    if path.is_dir():
                        shutil.rmtree(path, ignore_errors=True)
                    else:
                        path.unlink()
                    removed_files.append(str(path))
                except Exception:
                    pass

        base_dir = get_user_file_path("", "readings")
        folder = documents[doc_id].get("folder", "")
        file_path = base_dir / folder / filename

        # ç›¸å…³æ–‡ä»¶è·¯å¾„
        content_path = get_user_file_path(f"{doc_id}_content.json", "readings")
        notes_path = get_user_file_path(f"{doc_id}_notes.json", "readings")
        converted_pdf_path = Path(converted_pdf) if converted_pdf else None

        # EPUB å›¾ç‰‡ç›®å½•ï¼ˆä¸æå–æ—¶ä¸€è‡´çš„ hash è§„åˆ™ï¼‰
        epub_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        images_dir = USER_DATA_DIR / "readings" / "reading_images" / epub_hash

        # åˆ é™¤æ–‡ä»¶/ç›®å½•
        for p in [file_path, content_path, notes_path, images_dir, converted_pdf_path]:
            if p:
                remove_path(Path(p))

        return jsonify({
            "status": "success",
            "removed": removed_files
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/documents", methods=["GET"])
def list_documents():
    """åˆ—å‡ºæ‰€æœ‰å·²åŠ è½½çš„æ–‡æ¡£"""
    try:
        doc_index_path = get_user_file_path("documents.json", "readings")
        
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
        
        return jsonify({
            "status": "success",
            "documents": documents
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/create-folder", methods=["POST"])
def create_folder():
    """åˆ›å»ºæ–°æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        folder_name = payload.get("folder_name")
        parent_path = payload.get("parent_path", "")
        
        if not folder_name:
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "readings")
        folder_path = base_dir / parent_path / folder_name
        
        if folder_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹å·²å­˜åœ¨"}), 400
        
        folder_path.mkdir(parents=True, exist_ok=True)
        
        return jsonify({
            "status": "success",
            "folder_path": str(folder_path.relative_to(base_dir))
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/delete-folder", methods=["POST"])
def delete_folder():
    """åˆ é™¤æ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        folder_path = payload.get("folder_path")
        
        if not folder_path:
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "readings")
        folder_full_path = base_dir / folder_path
        
        if not folder_full_path.exists() or not folder_full_path.is_dir():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 404
        
        import shutil
        shutil.rmtree(folder_full_path)
        
        return jsonify({
            "status": "success",
            "message": "æ–‡ä»¶å¤¹åˆ é™¤æˆåŠŸ"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/rename-folder", methods=["POST"])
def rename_folder():
    """é‡å‘½åæ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        old_path = payload.get("old_path")
        new_name = payload.get("new_name")
        
        if not old_path or not new_name:
            return jsonify({"status": "error", "error": "å‚æ•°ä¸èƒ½ä¸ºç©º"}), 400
        
        base_dir = get_user_file_path("", "readings")
        old_full_path = base_dir / old_path
        new_full_path = old_full_path.parent / new_name
        
        if not old_full_path.exists() or not old_full_path.is_dir():
            return jsonify({"status": "error", "error": "æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 404
        
        if new_full_path.exists():
            return jsonify({"status": "error", "error": "æ–°åç§°å·²å­˜åœ¨"}), 400
        
        old_full_path.rename(new_full_path)
        
        return jsonify({
            "status": "success",
            "new_path": str(new_full_path.relative_to(base_dir))
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/list-folders", methods=["GET"])
def list_folders():
    """åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶å¤¹"""
    try:
        base_dir = get_user_file_path("", "readings")
        
        folders = []
        for root, dirs, files in os.walk(base_dir):
            for dir_name in dirs:
                folder_path = Path(root) / dir_name
                relative_path = str(folder_path.relative_to(base_dir))
                folders.append({
                    "path": relative_path,
                    "name": dir_name
                })
        
        return jsonify({
            "status": "success",
            "folders": folders
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/move-document", methods=["POST"])
def move_document():
    """ç§»åŠ¨æ–‡æ¡£åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
    try:
        payload = request.get_json(force=True)
        doc_id = payload.get("doc_id")
        target_folder = payload.get("target_folder")
        
        if not doc_id or not target_folder:
            return jsonify({"status": "error", "error": "å‚æ•°ä¸èƒ½ä¸ºç©º"}), 400
        
        # åŠ è½½æ–‡æ¡£ç´¢å¼•
        doc_index_path = get_user_file_path("documents.json", "readings")
        documents = {}
        if doc_index_path.exists():
            with open(doc_index_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
        
        if doc_id not in documents:
            return jsonify({"status": "error", "error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404
        
        # è·å–æ–‡æ¡£ä¿¡æ¯
        doc_info = documents[doc_id]
        filename = doc_info.get("filename")
        
        # ç§»åŠ¨æ–‡ä»¶
        base_dir = get_user_file_path("", "readings")
        old_file_path = base_dir / filename
        new_file_path = base_dir / target_folder / filename
        
        if not old_file_path.exists():
            return jsonify({"status": "error", "error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
        
        # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
        new_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç§»åŠ¨æ–‡ä»¶
        old_file_path.rename(new_file_path)
        
        # æ›´æ–°æ–‡æ¡£ç´¢å¼•ä¸­çš„è·¯å¾„ä¿¡æ¯
        doc_info["folder"] = target_folder
        
        with open(doc_index_path, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            "status": "success",
            "message": "æ–‡æ¡£ç§»åŠ¨æˆåŠŸ"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/save-notes/<doc_id>", methods=["POST"])
def save_reading_notes(doc_id):
    """ä¿å­˜é˜…è¯»ç¬”è®°"""
    try:
        payload = request.get_json(force=True)
        notes = payload.get("notes", [])
        
        notes_path = get_user_file_path(f"{doc_id}_notes.json", "readings")
        with open(notes_path, 'w', encoding='utf-8') as f:
            json.dump(notes, f, ensure_ascii=False, indent=2)
        
        return jsonify({"status": "success", "path": str(notes_path)})
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/load-notes/<doc_id>", methods=["GET"])
def load_reading_notes(doc_id):
    """åŠ è½½é˜…è¯»ç¬”è®°"""
    try:
        notes_path = get_user_file_path(f"{doc_id}_notes.json", "readings")
        
        notes = []
        if notes_path.exists():
            with open(notes_path, 'r', encoding='utf-8') as f:
                notes = json.load(f)
        
        return jsonify({
            "status": "success",
            "notes": notes
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/extract-words/<doc_id>", methods=["GET"])
def extract_document_words(doc_id):
    """ä»æ–‡æ¡£ä¸­æå–è¯æ±‡å¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    try:
        content_path = get_user_file_path(f"{doc_id}_content.json", "readings")
        
        if not content_path.exists():
            return jsonify({"status": "error", "error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404
        
        with open(content_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        text = data.get("text", "")
        words = extract_words_from_text(text)
        
        # è®¡ç®—è¯é¢‘
        word_count = {}
        for word in words:
            word_count[word] = text.lower().count(word)
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æ€»è¯æ•°
        total_words = count_total_words(text)
        
        return jsonify({
            "status": "success",
            "words": [{"word": w, "count": c} for w, c in sorted_words[:100]],
            "total_unique": len(words),
            "total_words": total_words,
            "text_length": len(text)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/reading/search/<doc_id>", methods=["POST"])
def search_in_document(doc_id):
    """åœ¨æ–‡æ¡£ä¸­æœç´¢æ–‡æœ¬"""
    try:
        payload = request.get_json(force=True)
        query = payload.get("query", "").lower()
        
        if not query:
            return jsonify({"status": "error", "error": "æœç´¢è¯ä¸ºç©º"}), 400
        
        content_path = get_user_file_path(f"{doc_id}_content.json", "readings")
        
        if not content_path.exists():
            return jsonify({"status": "error", "error": "æ–‡æ¡£ä¸å­˜åœ¨"}), 404
        
        with open(content_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        text = data.get("text", "")
        text_lower = text.lower()
        
        results = []
        pos = 0
        while True:
            idx = text_lower.find(query, pos)
            if idx == -1:
                break
            
            # è·å–ä¸Šä¸‹æ–‡
            context_start = max(0, idx - 50)
            context_end = min(len(text), idx + len(query) + 50)
            context = text[context_start:context_end]
            
            # è®¡ç®—å­—ç¬¦ä½ç½®ç™¾åˆ†æ¯”
            char_percent = round((idx / len(text)) * 100) if len(text) > 0 else 0
            
            results.append({
                "position": idx,
                "char_percent": char_percent,
                "context": context
            })
            pos = idx + 1
        
        return jsonify({
            "status": "success",
            "query": query,
            "results": results,
            "count": len(results)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


# ============================================================================
# PDFç¼“å­˜API - ä¿å­˜å’ŒåŠ è½½PDFæ¸²æŸ“æ•°æ®ï¼Œé¿å…é‡å¤æ¸²æŸ“
# ============================================================================

PDF_CACHE_DIR = Path(__file__).parent.parent / "user_data" / "pdf_cache"

def get_pdf_cache_path(pdf_filename: str) -> Path:
    """è·å–PDFç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    PDF_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    # ä½¿ç”¨PDFæ–‡ä»¶åï¼ˆå»é™¤è·¯å¾„ï¼‰ä½œä¸ºç¼“å­˜æ–‡ä»¶å
    cache_filename = Path(pdf_filename).stem + ".cache.json"
    return PDF_CACHE_DIR / cache_filename


@app.route("/api/pdf-cache/save", methods=["POST"])
def save_pdf_cache():
    """ä¿å­˜PDFæ¸²æŸ“æ•°æ®åˆ°ç¼“å­˜
    
    è¯·æ±‚ä½“:
    {
        "pdfFilename": "example.pdf",  # PDFæ–‡ä»¶åï¼ˆç”¨äºè¯†åˆ«ç¼“å­˜ï¼‰
        "currentPage": 5,               # å½“å‰é¡µç 
        "scale": 1.2,                   # ç¼©æ”¾çº§åˆ«ï¼ˆæ•°å­—æˆ–'auto', 'fit-width', 'fit-page'ï¼‰
        "scrollTop": 1024,              # æ»šåŠ¨ä½ç½®
        "displayMode": "continuous"     # æ˜¾ç¤ºæ¨¡å¼
    }
    """
    try:
        data = request.get_json()
        pdf_filename = data.get("pdfFilename", "")
        
        if not pdf_filename:
            return jsonify({"status": "error", "message": "pdfFilename is required"}), 400
        
        cache_path = get_pdf_cache_path(pdf_filename)
        
        # å‡†å¤‡ç¼“å­˜æ•°æ®
        cache_data = {
            "pdfFilename": pdf_filename,
            "currentPage": data.get("currentPage", 1),
            "scale": data.get("scale", "auto"),
            "scrollTop": data.get("scrollTop", 0),
            "displayMode": data.get("displayMode", "continuous"),
            "timestamp": int(__import__("time").time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³
        }
        
        # å†™å…¥ç¼“å­˜æ–‡ä»¶
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            "status": "success",
            "message": f"PDF cache saved for {pdf_filename}",
            "cachePath": str(cache_path)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/pdf-cache/load", methods=["POST"])
def load_pdf_cache():
    """åŠ è½½PDFæ¸²æŸ“ç¼“å­˜
    
    è¯·æ±‚ä½“:
    {
        "pdfFilename": "example.pdf"
    }
    
    è¿”å›ç¼“å­˜çš„é¡µé¢ã€ç¼©æ”¾ã€æ»šåŠ¨ä½ç½®ç­‰ä¿¡æ¯
    """
    try:
        data = request.get_json()
        pdf_filename = data.get("pdfFilename", "")
        
        if not pdf_filename:
            return jsonify({"status": "error", "message": "pdfFilename is required"}), 400
        
        cache_path = get_pdf_cache_path(pdf_filename)
        
        # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œè¯»å–å¹¶è¿”å›
        if cache_path.exists():
            with open(cache_path, "r", encoding="utf-8") as f:
                cache_data = json.load(f)
            
            return jsonify({
                "status": "success",
                "found": True,
                "cache": cache_data
            })
        else:
            return jsonify({
                "status": "success",
                "found": False,
                "cache": None,
                "message": "No cache found for this PDF"
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/pdf-cache/delete", methods=["POST"])
def delete_pdf_cache():
    """åˆ é™¤PDFæ¸²æŸ“ç¼“å­˜
    
    è¯·æ±‚ä½“:
    {
        "pdfFilename": "example.pdf"
    }
    """
    try:
        data = request.get_json()
        pdf_filename = data.get("pdfFilename", "")
        
        if not pdf_filename:
            return jsonify({"status": "error", "message": "pdfFilename is required"}), 400
        
        cache_path = get_pdf_cache_path(pdf_filename)
        
        if cache_path.exists():
            cache_path.unlink()
            return jsonify({
                "status": "success",
                "message": f"Cache deleted for {pdf_filename}"
            })
        else:
            return jsonify({
                "status": "success",
                "message": "No cache found to delete"
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/pdf-cache/list", methods=["GET"])
def list_pdf_cache():
    """åˆ—å‡ºæ‰€æœ‰PDFç¼“å­˜"""
    try:
        if not PDF_CACHE_DIR.exists():
            return jsonify({
                "status": "success",
                "caches": []
            })
        
        caches = []
        for cache_file in PDF_CACHE_DIR.glob("*.cache.json"):
            with open(cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)
            caches.append(cache_data)
        
        return jsonify({
            "status": "success",
            "caches": caches,
            "count": len(caches)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


# ============================================================================
# é€šç”¨æ–‡æ¡£é˜…è¯»è¿›åº¦API - ä¿å­˜æ‰€æœ‰æ–‡æ¡£ç±»å‹çš„é˜…è¯»è¿›åº¦
# ============================================================================

DOC_PROGRESS_DIR = Path(__file__).parent.parent / "user_data" / "doc_progress"

def get_doc_progress_path(doc_id: str) -> Path:
    """è·å–æ–‡æ¡£è¿›åº¦æ–‡ä»¶è·¯å¾„"""
    DOC_PROGRESS_DIR.mkdir(parents=True, exist_ok=True)
    # ä½¿ç”¨æ–‡æ¡£IDï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰ä½œä¸ºè¿›åº¦æ–‡ä»¶å
    safe_id = "".join(c if c.isalnum() or c in ('-', '_') else '_' for c in doc_id)
    progress_filename = safe_id + ".progress.json"
    return DOC_PROGRESS_DIR / progress_filename


@app.route("/api/doc-progress/save", methods=["POST"])
def save_doc_progress():
    """ä¿å­˜æ–‡æ¡£é˜…è¯»è¿›åº¦
    
    è¯·æ±‚ä½“:
    {
        "docId": "document-id",           # æ–‡æ¡£å”¯ä¸€IDï¼ˆå¿…éœ€ï¼‰
        "docType": "pdf|epub|docx|txt",  # æ–‡æ¡£ç±»å‹
        "scrollPosition": 1024,           # æ»šåŠ¨ä½ç½®ï¼ˆåƒç´ ï¼‰æˆ–å­—ç¬¦ä½ç½®
        "scrollPercent": 45.5,            # æ»šåŠ¨è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰
        "currentPage": 5,                 # å½“å‰é¡µç 
        "displayMode": "continuous",      # æ˜¾ç¤ºæ¨¡å¼
        "timestamp": 1234567890000        # æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    }
    """
    try:
        data = request.get_json()
        doc_id = data.get("docId", "")
        
        if not doc_id:
            return jsonify({"status": "error", "message": "docId is required"}), 400
        
        progress_path = get_doc_progress_path(doc_id)
        
        # å‡†å¤‡è¿›åº¦æ•°æ®
        progress_data = {
            "docId": doc_id,
            "docType": data.get("docType", "unknown"),
            "scrollPosition": data.get("scrollPosition", 0),
            "scrollPercent": data.get("scrollPercent", 0),
            "currentPage": data.get("currentPage", 1),
            "displayMode": data.get("displayMode", "continuous"),
            "timestamp": int(__import__("time").time() * 1000)
        }
        
        # å†™å…¥è¿›åº¦æ–‡ä»¶
        with open(progress_path, "w", encoding="utf-8") as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            "status": "success",
            "message": f"Progress saved for document {doc_id}",
            "progressPath": str(progress_path)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/doc-progress/load", methods=["POST"])
def load_doc_progress():
    """åŠ è½½æ–‡æ¡£é˜…è¯»è¿›åº¦
    
    è¯·æ±‚ä½“:
    {
        "docId": "document-id"
    }
    """
    try:
        data = request.get_json()
        doc_id = data.get("docId", "")
        
        if not doc_id:
            return jsonify({"status": "error", "message": "docId is required"}), 400
        
        progress_path = get_doc_progress_path(doc_id)
        
        if progress_path.exists():
            with open(progress_path, "r", encoding="utf-8") as f:
                progress_data = json.load(f)
            
            return jsonify({
                "status": "success",
                "found": True,
                "progress": progress_data
            })
        else:
            return jsonify({
                "status": "success",
                "found": False,
                "progress": None,
                "message": "No progress found for this document"
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/doc-progress/delete", methods=["POST"])
def delete_doc_progress():
    """åˆ é™¤æ–‡æ¡£é˜…è¯»è¿›åº¦"""
    try:
        data = request.get_json()
        doc_id = data.get("docId", "")
        
        if not doc_id:
            return jsonify({"status": "error", "message": "docId is required"}), 400
        
        progress_path = get_doc_progress_path(doc_id)
        
        if progress_path.exists():
            progress_path.unlink()
            return jsonify({
                "status": "success",
                "message": f"Progress deleted for document {doc_id}"
            })
        else:
            return jsonify({
                "status": "success",
                "message": "No progress found to delete"
            })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/doc-progress/list", methods=["GET"])
def list_doc_progress():
    """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£é˜…è¯»è¿›åº¦"""
    try:
        if not DOC_PROGRESS_DIR.exists():
            return jsonify({
                "status": "success",
                "progresses": []
            })
        
        progresses = []
        for progress_file in DOC_PROGRESS_DIR.glob("*.progress.json"):
            with open(progress_file, "r", encoding="utf-8") as f:
                progress_data = json.load(f)
            progresses.append(progress_data)
        
        return jsonify({
            "status": "success",
            "progresses": progresses,
            "count": len(progresses)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/")
def index():
    idx = Path(__file__).parent.parent / "static" / "index.html"
    if idx.exists():
        return send_file(str(idx))
    else:
        return jsonify({"status": "error", "message": "index.html not found", "path": str(idx)}), 500


@app.route("/static/pdf-viewer.html")
def pdf_viewer():
    """PDFæŸ¥çœ‹å™¨è·¯ç”± - ç¦ç”¨ç¼“å­˜ç¡®ä¿æ€»æ˜¯ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬"""
    viewer_path = Path(__file__).parent.parent / "static" / "pdf-viewer.html"
    if viewer_path.exists():
        response = send_file(str(viewer_path))
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        return response
    else:
        return jsonify({"status": "error", "message": "pdf-viewer.html not found"}), 404


# ============================================================================
# è¯å…¸æŸ¥è¯¢API - æ”¯æŒè¯åº“ç®¡ç†å’Œpymorphy2è¯æ³•åˆ†æ
# ============================================================================

# åˆå§‹åŒ–pymorphy2åˆ†æå™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_morph_analyzer = None

def get_morph_analyzer():
    """è·å–pymorphy2åˆ†æå™¨å®ä¾‹"""
    global _morph_analyzer
    if _morph_analyzer is None and pymorphy2 is not None:
        try:
            _morph_analyzer = pymorphy2.MorphAnalyzer()
            print("âœ“ pymorphy2åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— pymorphy2åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            _morph_analyzer = None
    return _morph_analyzer


def normalize_word(word: str) -> List[str]:
    """ä½¿ç”¨pymorphy2å°†è¯æ±‡è¿˜åŸä¸ºåŸå½¢
    
    è¿”å›å¯èƒ½çš„åŸå½¢åˆ—è¡¨
    """
    morph = get_morph_analyzer()
    if morph is None:
        return [word.lower()]
    
    try:
        parses = morph.parse(word)
        normal_forms = list(set([p.normal_form for p in parses]))
        return normal_forms if normal_forms else [word.lower()]
    except Exception as e:
        print(f"è¯æ³•åˆ†æå¤±è´¥ {word}: {e}")
        return [word.lower()]


def analyze_word_morphology(word: str) -> Dict[str, Any]:
    """åˆ†æè¯æ±‡çš„è¯æ³•ä¿¡æ¯
    
    è¿”å›ï¼š
    - normal_form: åŸå½¢
    - pos: è¯æ€§
    - case: æ ¼
    - gender: æ€§
    - number: æ•°
    - tense: æ—¶æ€
    - person: äººç§°
    - voice: è¯­æ€
    - mood: å¼
    - aspect: ä½“
    - animacy: æœ‰ç”Ÿå‘½æ€§
    - transitivity: åŠç‰©æ€§
    - involvement: å‚ä¸æ€§
    """
    morph = get_morph_analyzer()
    if morph is None:
        return {"word": word, "normal_forms": [word.lower()], "analyses": []}
    
    try:
        parses = morph.parse(word)
        analyses = []
        
        for p in parses:
            analysis = {
                "normal_form": p.normal_form,
                "pos": str(p.tag.POS) if p.tag.POS else None,
                "case": str(p.tag.case) if hasattr(p.tag, 'case') and p.tag.case else None,
                "gender": str(p.tag.gender) if hasattr(p.tag, 'gender') and p.tag.gender else None,
                "number": str(p.tag.number) if hasattr(p.tag, 'number') and p.tag.number else None,
                "tense": str(p.tag.tense) if hasattr(p.tag, 'tense') and p.tag.tense else None,
                "person": str(p.tag.person) if hasattr(p.tag, 'person') and p.tag.person else None,
                "voice": str(p.tag.voice) if hasattr(p.tag, 'voice') and p.tag.voice else None,
                "mood": str(p.tag.mood) if hasattr(p.tag, 'mood') and p.tag.mood else None,
                "aspect": str(p.tag.aspect) if hasattr(p.tag, 'aspect') and p.tag.aspect else None,
                "animacy": str(p.tag.animacy) if hasattr(p.tag, 'animacy') and p.tag.animacy else None,
                "transitivity": str(p.tag.transitivity) if hasattr(p.tag, 'transitivity') and p.tag.transitivity else None,
                "involvement": str(p.tag.involvement) if hasattr(p.tag, 'involvement') and p.tag.involvement else None,
                "score": float(p.score),
                "tag": str(p.tag)
            }
            analyses.append(analysis)
        
        return {
            "word": word,
            "normal_forms": list(set([p.normal_form for p in parses])),
            "analyses": analyses
        }
    except Exception as e:
        print(f"è¯æ³•åˆ†æå¤±è´¥ {word}: {e}")
        return {"word": word, "normal_forms": [word.lower()], "analyses": [], "error": str(e)}


def generate_word_inflections(word: str) -> Dict[str, Any]:
    """ç”Ÿæˆè¯æ±‡çš„å˜æ ¼å½¢å¼
    
    è¿”å›ï¼š
    - word: åŸå§‹è¯æ±‡
    - normal_form: åŸå½¢
    - inflections: å˜æ ¼å½¢å¼åˆ—è¡¨ï¼ŒæŒ‰è¯æ€§åˆ†ç»„
    """
    morph = get_morph_analyzer()
    if morph is None:
        return {"word": word, "normal_form": word.lower(), "inflections": {}}
    
    try:
        parses = morph.parse(word)
        if not parses:
            return {"word": word, "normal_form": word.lower(), "inflections": {}}
        
        # æŒ‰è¯æ€§åˆ†ç»„è§£æç»“æœ
        pos_groups = {}
        for p in parses:
            pos = p.tag.POS
            if pos:
                pos_str = str(pos)
                if pos_str not in pos_groups:
                    pos_groups[pos_str] = []
                pos_groups[pos_str].append(p)
        
        # ä¸ºæ¯ä¸ªè¯æ€§ç”Ÿæˆå˜æ ¼å½¢å¼
        inflections = {}
        
        for pos_str, pos_parses in pos_groups.items():
            best_parse = pos_parses[0]
            normal_form = best_parse.normal_form
            
            if pos_str == 'INFN':
                # å°è¯•è·å–ç¬¬ä¸€äººç§°å•æ•°å½¢å¼
                verb_parse = best_parse
                try:
                    verb_form = best_parse.inflect({'1per', 'sing'})
                    if verb_form:
                        # é‡æ–°è§£æåŠ¨è¯å½¢å¼
                        verb_parses = morph.parse(verb_form.word)
                        if verb_parses:
                            verb_parse = verb_parses[0]
                            pos = verb_parse.tag.POS
                            pos_str = str(pos) if pos else None
                except:
                    pass
                
                # å¦‚æœè¿˜æ˜¯ INFNï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
                if pos_str == 'INFN':
                    pos_inflections = generate_generic_inflections(best_parse)
                else:
                    pos_inflections = generate_verb_inflections(verb_parse)
            elif pos_str == 'NOUN':
                pos_inflections = generate_noun_inflections(best_parse)
            elif pos_str == 'VERB':
                pos_inflections = generate_verb_inflections(best_parse)
            elif pos_str == 'ADJF':
                pos_inflections = generate_adjective_inflections(best_parse)
            elif pos_str == 'ADJS':
                pos_inflections = generate_adjective_inflections(best_parse)
            elif pos_str == 'NUMR':
                pos_inflections = generate_numeral_inflections(best_parse)
            elif pos_str == 'NPRO':
                pos_inflections = generate_pronoun_inflections(best_parse)
            else:
                pos_inflections = generate_generic_inflections(best_parse)
            
            inflections[pos_str] = {
                "normal_form": normal_form,
                "inflections": pos_inflections
            }
        
        return {
            "word": word,
            "normal_form": parses[0].normal_form,
            "inflections": inflections
        }
    except Exception as e:
        print(f"å˜æ ¼å½¢å¼ç”Ÿæˆå¤±è´¥ {word}: {e}")
        return {"word": word, "normal_form": word.lower(), "inflections": {}, "error": str(e)}


def generate_noun_inflections(parse) -> Dict[str, Any]:
    """ç”Ÿæˆåè¯çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
    case_names = {
        'nomn': 'ä¸»æ ¼',
        'gent': 'å±æ ¼',
        'datv': 'ä¸æ ¼',
        'accs': 'å®¾æ ¼',
        'ablt': 'å·¥å…·æ ¼',
        'loct': 'å‰ç½®æ ¼'
    }
    
    numbers = ['sing', 'plur']
    number_names = {
        'sing': 'å•æ•°',
        'plur': 'å¤æ•°'
    }
    
    inflections = {}
    
    for num in numbers:
        num_key = number_names.get(num, num)
        inflections[num_key] = {}
        
        for case in cases:
            try:
                if num == 'sing':
                    inflected = parse.inflect({case})
                else:
                    inflected = parse.inflect({case, num})
                
                if inflected:
                    case_key = case_names.get(case, case)
                    inflections[num_key][case_key] = inflected.word
            except:
                pass
    
    return inflections


def generate_verb_inflections(parse) -> Dict[str, Any]:
    """ç”ŸæˆåŠ¨è¯çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    inflections = {}
    
    # ä¸å®šå¼
    infinitive = parse.normal_form
    
    # ä¸»åŠ¨è¯­æ€
    active_voice = {}
    
    # ç°åœ¨/å°†æ¥æ—¶
    present_future = {}
    tenses = ['pres', 'futr']
    persons = ['1per', '2per', '3per']
    person_names = {
        '1per': 'ä¸€',
        '2per': 'äºŒ',
        '3per': 'ä¸‰'
    }
    numbers = ['sing', 'plur']
    number_names = {
        'sing': 'å•æ•°',
        'plur': 'å¤æ•°'
    }
    
    for tense in tenses:
        tense_key = 'ç°åœ¨æ—¶' if tense == 'pres' else 'å°†æ¥æ—¶'
        present_future[tense_key] = {}
        
        for num in numbers:
            num_key = number_names.get(num, num)
            present_future[tense_key][num_key] = {}
            
            for person in persons:
                try:
                    inflected = parse.inflect({tense, person, num})
                    if inflected:
                        person_key = person_names.get(person, person)
                        present_future[tense_key][num_key][person_key] = inflected.word
                except:
                    pass
    
    active_voice['ç°åœ¨/å°†æ¥æ—¶'] = present_future
    
    # è¿‡å»æ—¶
    past_tense = {}
    genders = ['masc', 'femn', 'neut']
    gender_names = {
        'masc': 'é˜³æ€§',
        'femn': 'é˜´æ€§',
        'neut': 'ä¸­æ€§'
    }
    
    for gender in genders:
        gender_key = gender_names.get(gender, gender)
        try:
            inflected = parse.inflect({'past', gender})
            if inflected:
                past_tense[gender_key] = inflected.word
        except:
            pass
    
    # å¤æ•°è¿‡å»æ—¶
    try:
        inflected = parse.inflect({'past', 'plur'})
        if inflected:
            past_tense['å¤æ•°'] = inflected.word
    except:
        pass
    
    active_voice['è¿‡å»æ—¶'] = past_tense
    
    # å‰¯åŠ¨è¯
    adverbial_participle = []
    try:
        inflected = parse.inflect({'GRND', 'past'})
        if inflected:
            adverbial_participle.append(inflected.word)
    except:
        pass
    
    try:
        inflected = parse.inflect({'GRND', 'pres'})
        if inflected:
            adverbial_participle.append(inflected.word)
    except:
        pass
    
    if adverbial_participle:
        active_voice['å‰¯åŠ¨è¯'] = ' // '.join(adverbial_participle)
    
    # å‘½ä»¤å¼
    imperative = {}
    for num in numbers:
        num_key = number_names.get(num, num)
        try:
            inflected = parse.inflect({'impr', num})
            if inflected:
                imperative[num_key] = inflected.word
        except:
            pass
    
    active_voice['å‘½ä»¤å¼'] = imperative
    
    # è¿‡å»æ—¶ä¸»åŠ¨å½¢åŠ¨è¯
    past_active_participle = {}
    cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
    case_names = {
        'nomn': 'ä¸€æ ¼',
        'gent': 'äºŒæ ¼',
        'datv': 'ä¸‰æ ¼',
        'accs': 'å››æ ¼',
        'ablt': 'äº”æ ¼',
        'loct': 'å…­æ ¼'
    }
    
    for case in cases:
        case_key = case_names.get(case, case)
        past_active_participle[case_key] = {}
        
        for gender in genders:
            gender_key = gender_names.get(gender, gender)
            try:
                inflected = parse.inflect({'PRTF', 'past', 'actv', case, gender})
                if inflected:
                    past_active_participle[case_key][gender_key] = inflected.word
            except:
                pass
        
        # å¤æ•°
        try:
            inflected = parse.inflect({'PRTF', 'past', 'actv', case, 'plur'})
            if inflected:
                past_active_participle[case_key]['å¤æ•°'] = inflected.word
        except:
            pass
    
    active_voice['è¿‡å»æ—¶ä¸»åŠ¨å½¢åŠ¨è¯'] = past_active_participle
    
    inflections['ä¸»åŠ¨è¯­æ€'] = active_voice
    
    # è¢«åŠ¨è¯­æ€
    passive_voice = {}
    
    # è¿‡å»æ—¶è¢«åŠ¨å½¢åŠ¨è¯
    past_passive_participle = {}
    
    for case in cases:
        case_key = case_names.get(case, case)
        past_passive_participle[case_key] = {}
        
        for gender in genders:
            gender_key = gender_names.get(gender, gender)
            try:
                inflected = parse.inflect({'PRTF', 'past', 'pssv', case, gender})
                if inflected:
                    past_passive_participle[case_key][gender_key] = inflected.word
            except:
                pass
        
        # å¤æ•°
        try:
            inflected = parse.inflect({'PRTF', 'past', 'pssv', case, 'plur'})
            if inflected:
                past_passive_participle[case_key]['å¤æ•°'] = inflected.word
        except:
            pass
    
    passive_voice['è¿‡å»æ—¶è¢«åŠ¨å½¢åŠ¨è¯'] = past_passive_participle
    
    # ç®€ç•¥å½¢å¼
    short_form = {}
    for gender in genders:
        gender_key = gender_names.get(gender, gender)
        try:
            inflected = parse.inflect({'PRTF', 'past', 'pssv', 'shrt', gender})
            if inflected:
                short_form[gender_key] = inflected.word
        except:
            pass
    
    # å¤æ•°ç®€ç•¥å½¢å¼
    try:
        inflected = parse.inflect({'PRTF', 'past', 'pssv', 'shrt', 'plur'})
        if inflected:
            short_form['å¤æ•°'] = inflected.word
    except:
        pass
    
    if short_form:
        passive_voice['ç®€ç•¥å½¢å¼'] = short_form
    
    if passive_voice:
        inflections['è¢«åŠ¨è¯­æ€'] = passive_voice
    
    # æ·»åŠ ä¸å®šå¼
    inflections['ä¸å®šå¼'] = infinitive
    
    return inflections


def generate_adjective_inflections(parse) -> Dict[str, Any]:
    """ç”Ÿæˆå½¢å®¹è¯çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
    case_names = {
        'nomn': 'ä¸»æ ¼',
        'gent': 'å±æ ¼',
        'datv': 'ä¸æ ¼',
        'accs': 'å®¾æ ¼',
        'ablt': 'å·¥å…·æ ¼',
        'loct': 'å‰ç½®æ ¼'
    }
    
    genders = ['masc', 'femn', 'neut']
    gender_names = {
        'masc': 'é˜³æ€§',
        'femn': 'é˜´æ€§',
        'neut': 'ä¸­æ€§'
    }
    
    numbers = ['sing', 'plur']
    number_names = {
        'sing': 'å•æ•°',
        'plur': 'å¤æ•°'
    }
    
    inflections = {}
    
    for num in numbers:
        num_key = number_names.get(num, num)
        inflections[num_key] = {}
        
        if num == 'sing':
            for gender in genders:
                gender_key = gender_names.get(gender, gender)
                inflections[num_key][gender_key] = {}
                
                for case in cases:
                    try:
                        inflected = parse.inflect({case, gender, num})
                        if inflected:
                            case_key = case_names.get(case, case)
                            inflections[num_key][gender_key][case_key] = inflected.word
                    except:
                        pass
        else:
            for case in cases:
                try:
                    inflected = parse.inflect({case, num})
                    if inflected:
                        case_key = case_names.get(case, case)
                        inflections[num_key][case_key] = inflected.word
                except:
                        pass
    
    # çŸ­å°¾å½¢å¼
    short_forms = {}
    for gender in genders:
        gender_key = gender_names.get(gender, gender)
        try:
            inflected = parse.inflect({'ADJS', gender, 'sing'})
            if inflected:
                short_forms[gender_key] = inflected.word
        except:
            pass
    
    try:
        inflected = parse.inflect({'ADJS', 'plur'})
        if inflected:
            short_forms['å¤æ•°'] = inflected.word
    except:
        pass
    
    if short_forms:
        inflections['çŸ­å°¾å½¢å¼'] = short_forms
    
    # æ¯”è¾ƒçº§
    comparative_forms = []
    
    # å°è¯•ä»åŸå§‹è¯è·å–æ¯”è¾ƒçº§
    try:
        # è§£æåŸå§‹è¯
        base_word = parse.normal_form
        parses = morph.parse(base_word)
        
        # æŸ¥æ‰¾åŒ…å«æ¯”è¾ƒçº§æ ‡ç­¾çš„è§£æ
        for p in parses:
            tag_str = str(p.tag)
            if 'COMP' in tag_str or 'compar' in tag_str.lower():
                comparative_forms.append(p.word)
                break
    except:
        pass
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œå°è¯•ç›´æ¥ä½¿ç”¨inflect
    if not comparative_forms:
        try:
            inflected = parse.inflect({'COMP'})
            if inflected:
                comparative_forms.append(inflected.word)
        except:
            pass
    
    if comparative_forms:
        inflections['æ¯”è¾ƒçº§'] = ' // '.join(comparative_forms)
    
    return inflections


def generate_numeral_inflections(parse) -> Dict[str, Any]:
    """ç”Ÿæˆæ•°è¯çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
    case_names = {
        'nomn': 'ä¸»æ ¼',
        'gent': 'å±æ ¼',
        'datv': 'ä¸æ ¼',
        'accs': 'å®¾æ ¼',
        'ablt': 'å·¥å…·æ ¼',
        'loct': 'å‰ç½®æ ¼'
    }
    
    numbers = ['sing', 'plur']
    number_names = {
        'sing': 'å•æ•°',
        'plur': 'å¤æ•°'
    }
    
    inflections = {}
    
    for num in numbers:
        num_key = number_names.get(num, num)
        inflections[num_key] = {}
        
        for case in cases:
            try:
                inflected = parse.inflect({case, num})
                if inflected:
                    case_key = case_names.get(case, case)
                    inflections[num_key][case_key] = inflected.word
            except:
                pass
    
    return inflections


def generate_pronoun_inflections(parse) -> Dict[str, Any]:
    """ç”Ÿæˆä»£è¯çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
    case_names = {
        'nomn': 'ä¸»æ ¼',
        'gent': 'å±æ ¼',
        'datv': 'ä¸æ ¼',
        'accs': 'å®¾æ ¼',
        'ablt': 'å·¥å…·æ ¼',
        'loct': 'å‰ç½®æ ¼'
    }
    
    numbers = ['sing', 'plur']
    number_names = {
        'sing': 'å•æ•°',
        'plur': 'å¤æ•°'
    }
    
    inflections = {}
    
    for num in numbers:
        num_key = number_names.get(num, num)
        inflections[num_key] = {}
        
        for case in cases:
            try:
                inflected = parse.inflect({case, num})
                if inflected:
                    case_key = case_names.get(case, case)
                    inflections[num_key][case_key] = inflected.word
            except:
                pass
    
    return inflections


def generate_generic_inflections(parse) -> Dict[str, Any]:
    """ç”Ÿæˆé€šç”¨è¯æ±‡çš„å˜æ ¼å½¢å¼"""
    morph = get_morph_analyzer()
    if morph is None:
        return {}
    
    inflections = {}
    
    try:
        if parse.tag.case:
            cases = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']
            case_names = {
                'nomn': 'ä¸»æ ¼',
                'gent': 'å±æ ¼',
                'datv': 'ä¸æ ¼',
                'accs': 'å®¾æ ¼',
                'ablt': 'å·¥å…·æ ¼',
                'loct': 'å‰ç½®æ ¼'
            }
            
            for case in cases:
                try:
                    inflected = parse.inflect({case})
                    if inflected:
                        case_key = case_names.get(case, case)
                        inflections[case_key] = inflected.word
                except:
                    pass
    except:
        pass
    
    return inflections


def load_dictionary_files() -> List[Dict[str, Any]]:
    """åŠ è½½æ‰€æœ‰è¯åº“æ–‡ä»¶"""
    # åŠ è½½é¡¹ç›®å†…ç½®è¯å…¸ï¼ˆdata/dictionaryï¼‰
    builtin_dict_dir = Path(__file__).parent.parent / "data" / "dictionary"
    builtin_dict_dir.mkdir(exist_ok=True, parents=True)
    
    # åŠ è½½ç”¨æˆ·è¯å…¸ï¼ˆuser_data/dictionaryï¼‰
    user_dict_dir = get_user_file_path("", "dictionary")
    user_dict_dir.mkdir(exist_ok=True)
    
    dictionaries = []
    
    # å…ˆåŠ è½½å†…ç½®è¯å…¸
    for file_path in builtin_dict_dir.glob("*"):
        if file_path.is_file() and file_path.suffix.lower() in ['.json', '.csv', '.tsv', '.txt']:
            try:
                dict_data = {
                    "filename": file_path.name,
                    "path": str(file_path),
                    "size": file_path.stat().st_size,
                    "entries": [],
                    "type": "builtin"  # æ ‡è®°ä¸ºå†…ç½®è¯å…¸
                }
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½
                if file_path.suffix.lower() == '.json':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        dict_data["entries"] = json.load(f)
                elif file_path.suffix.lower() in ['.csv', '.tsv']:
                    import csv
                    delimiter = '\t' if file_path.suffix.lower() == '.tsv' else ','
                    with open(file_path, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f, delimiter=delimiter)
                        dict_data["entries"] = list(reader)
                
                dictionaries.append(dict_data)
            except Exception as e:
                print(f"åŠ è½½å†…ç½®è¯åº“å¤±è´¥ {file_path.name}: {e}")
    
    # å†åŠ è½½ç”¨æˆ·è¯å…¸
    for file_path in user_dict_dir.glob("*"):
        if file_path.is_file() and file_path.suffix.lower() in ['.json', '.csv', '.tsv', '.txt']:
            try:
                dict_data = {
                    "filename": file_path.name,
                    "path": str(file_path),
                    "size": file_path.stat().st_size,
                    "entries": [],
                    "type": "user"  # æ ‡è®°ä¸ºç”¨æˆ·è¯å…¸
                }
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½
                if file_path.suffix.lower() == '.json':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        dict_data["entries"] = json.load(f)
                elif file_path.suffix.lower() in ['.csv', '.tsv']:
                    import csv
                    delimiter = '\t' if file_path.suffix.lower() == '.tsv' else ','
                    with open(file_path, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f, delimiter=delimiter)
                        dict_data["entries"] = list(reader)
                
                dictionaries.append(dict_data)
            except Exception as e:
                print(f"åŠ è½½ç”¨æˆ·è¯åº“å¤±è´¥ {file_path.name}: {e}")
    
    return dictionaries


def search_in_dictionaries(word: str) -> List[Dict[str, Any]]:
    """åœ¨æ‰€æœ‰è¯åº“ä¸­æœç´¢è¯æ±‡"""
    # è·å–è¯æ±‡çš„åŸå½¢
    normal_forms = normalize_word(word)
    
    # åŠ è½½æ‰€æœ‰è¯åº“
    dictionaries = load_dictionary_files()
    
    results = []
    
    for dict_data in dictionaries:
        for entry in dict_data["entries"]:
            # æ£€æŸ¥entryæ˜¯å¦åŒ¹é…ï¼ˆåŸå½¢æˆ–å˜å½¢ï¼‰
            entry_word = entry.get("word", "").lower()
            
            if entry_word == word.lower() or entry_word in normal_forms:
                result = {
                    "source": dict_data["filename"],
                    "word": entry.get("word", ""),
                    "translation": entry.get("translation", ""),
                    "pos": entry.get("pos", ""),
                    "examples": entry.get("examples", []),
                    "notes": entry.get("notes", "")
                }
                results.append(result)
    
    return results


@app.route("/api/dictionary/lookup", methods=["POST"])
def dictionary_lookup():
    """æŸ¥è¯API
    
    è¯·æ±‚ä½“:
    {
        "word": "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚",
        "analyze": true  # æ˜¯å¦è¿›è¡Œè¯æ³•åˆ†æ
    }
    
    è¿”å›:
    {
        "status": "success",
        "word": "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚",
        "morphology": {...},  # è¯æ³•åˆ†æç»“æœ
        "dictionary": [...],  # è¯å…¸æŸ¥è¯¢ç»“æœ
        "vocab": {...}        # ç”Ÿè¯æœ¬ä¸­çš„è®°å½•
    }
    """
    try:
        data = request.get_json()
        word = data.get("word", "").strip()
        analyze = data.get("analyze", True)
        
        if not word:
            return jsonify({"status": "error", "error": "è¯æ±‡ä¸èƒ½ä¸ºç©º"}), 400
        
        result = {
            "status": "success",
            "word": word,
            "morphology": None,
            "dictionary": [],
            "vocab": None
        }
        
        # è¯æ³•åˆ†æ
        if analyze and pymorphy2 is not None:
            result["morphology"] = analyze_word_morphology(word)
            # ç”Ÿæˆå˜æ ¼å½¢å¼
            result["inflections"] = generate_word_inflections(word)
        
        # è¯å…¸æŸ¥è¯¢
        result["dictionary"] = search_in_dictionaries(word)
        
        # æŸ¥è¯¢ç”Ÿè¯æœ¬ï¼ˆä»æ‰€æœ‰ç”Ÿè¯æœ¬ä¸­æŸ¥æ‰¾ï¼‰
        vocabbooks_path = get_user_file_path("vocabbooks.json", "vocab")
        if vocabbooks_path.exists():
            with open(vocabbooks_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                vocabBooks = data.get("vocabBooks", [])
                
                for book in vocabBooks:
                    for vocab_word in book.get("words", []):
                        if vocab_word.get("word", "").lower() == word.lower():
                            result["vocab"] = vocab_word
                            break
                    if result["vocab"]:
                        break
        
        return jsonify(result)
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/dictionary/upload", methods=["POST"])
def upload_dictionary():
    """ä¸Šä¼ è¯åº“æ–‡ä»¶
    
    æ”¯æŒJSON, CSV, TSVæ ¼å¼
    """
    try:
        if 'file' not in request.files:
            return jsonify({"status": "error", "error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({"status": "error", "error": "æ–‡ä»¶åä¸ºç©º"}), 400
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        file_ext = Path(file.filename).suffix.lower()
        if file_ext not in ['.json', '.csv', '.tsv', '.txt']:
            return jsonify({"status": "error", "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}), 400
        
        # ä¿å­˜åˆ°dictionaryæ–‡ä»¶å¤¹
        dict_dir = get_user_file_path("", "dictionary")
        dict_dir.mkdir(exist_ok=True)
        
        file_path = dict_dir / file.filename
        file.save(str(file_path))
        
        # éªŒè¯æ–‡ä»¶å†…å®¹
        entry_count = 0
        try:
            if file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    entries = json.load(f)
                    entry_count = len(entries) if isinstance(entries, list) else 0
            elif file_ext in ['.csv', '.tsv']:
                import csv
                delimiter = '\t' if file_ext == '.tsv' else ','
                with open(file_path, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f, delimiter=delimiter)
                    entry_count = sum(1 for _ in reader)
        except Exception as e:
            file_path.unlink()  # åˆ é™¤æ— æ•ˆæ–‡ä»¶
            return jsonify({"status": "error", "error": f"æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}"}), 400
        
        return jsonify({
            "status": "success",
            "filename": file.filename,
            "entries": entry_count,
            "message": f"æˆåŠŸå¯¼å…¥ {entry_count} ä¸ªè¯æ¡"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/dictionary/list", methods=["GET"])
def list_dictionaries():
    """åˆ—å‡ºæ‰€æœ‰å·²å¯¼å…¥çš„è¯åº“"""
    try:
        # åˆ—å‡ºå†…ç½®è¯å…¸
        builtin_dict_dir = Path(__file__).parent.parent / "data" / "dictionary"
        builtin_dict_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆ—å‡ºç”¨æˆ·è¯å…¸
        user_dict_dir = get_user_file_path("", "dictionary")
        user_dict_dir.mkdir(exist_ok=True)
        
        dictionaries = []
        
        # æ·»åŠ å†…ç½®è¯å…¸
        for file_path in builtin_dict_dir.glob("*"):
            if file_path.is_file() and file_path.suffix.lower() in ['.json', '.csv', '.tsv', '.txt']:
                entry_count = 0
                try:
                    if file_path.suffix.lower() == '.json':
                        with open(file_path, 'r', encoding='utf-8') as f:
                            entries = json.load(f)
                            entry_count = len(entries) if isinstance(entries, list) else 0
                    elif file_path.suffix.lower() in ['.csv', '.tsv']:
                        import csv
                        delimiter = '\t' if file_path.suffix.lower() == '.tsv' else ','
                        with open(file_path, 'r', encoding='utf-8') as f:
                            reader = csv.DictReader(f, delimiter=delimiter)
                            entry_count = sum(1 for _ in reader)
                except:
                    entry_count = 0
                
                dictionaries.append({
                    "filename": file_path.name,
                    "size": file_path.stat().st_size,
                    "entries": entry_count,
                    "upload_time": file_path.stat().st_mtime,
                    "type": "builtin",
                    "editable": False  # å†…ç½®è¯å…¸ä¸å¯ç¼–è¾‘
                })
        
        # æ·»åŠ ç”¨æˆ·è¯å…¸
        for file_path in user_dict_dir.glob("*"):
            if file_path.is_file() and file_path.suffix.lower() in ['.json', '.csv', '.tsv', '.txt']:
                entry_count = 0
                try:
                    if file_path.suffix.lower() == '.json':
                        with open(file_path, 'r', encoding='utf-8') as f:
                            entries = json.load(f)
                            entry_count = len(entries) if isinstance(entries, list) else 0
                    elif file_path.suffix.lower() in ['.csv', '.tsv']:
                        import csv
                        delimiter = '\t' if file_path.suffix.lower() == '.tsv' else ','
                        with open(file_path, 'r', encoding='utf-8') as f:
                            reader = csv.DictReader(f, delimiter=delimiter)
                            entry_count = sum(1 for _ in reader)
                except:
                    entry_count = 0
                
                dictionaries.append({
                    "filename": file_path.name,
                    "size": file_path.stat().st_size,
                    "entries": entry_count,
                    "upload_time": file_path.stat().st_mtime,
                    "type": "user",
                    "editable": True  # ç”¨æˆ·è¯å…¸å¯ç¼–è¾‘
                })
        
        return jsonify({
            "status": "success",
            "dictionaries": dictionaries,
            "count": len(dictionaries)
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/dictionary/delete/<filename>", methods=["DELETE"])
def delete_dictionary(filename):
    """åˆ é™¤è¯åº“æ–‡ä»¶"""
    try:
        # åªå…è®¸åˆ é™¤ç”¨æˆ·è¯å…¸ï¼Œä¸å…è®¸åˆ é™¤å†…ç½®è¯å…¸
        dict_dir = get_user_file_path("", "dictionary")
        file_path = dict_dir / filename
        
        if not file_path.exists():
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…ç½®è¯å…¸
            builtin_dict_dir = Path(__file__).parent.parent / "data" / "dictionary"
            builtin_file_path = builtin_dict_dir / filename
            if builtin_file_path.exists():
                return jsonify({"status": "error", "error": "å†…ç½®è¯å…¸ä¸å¯åˆ é™¤"}), 403
            return jsonify({"status": "error", "error": "è¯åº“ä¸å­˜åœ¨"}), 404
        
        file_path.unlink()
        
        return jsonify({
            "status": "success",
            "message": f"å·²åˆ é™¤è¯åº“: {filename}"
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


@app.route("/api/dictionary/analyze/<word>", methods=["GET"])
def analyze_word(word):
    """åˆ†æå•ä¸ªè¯æ±‡çš„è¯æ³•ä¿¡æ¯"""
    try:
        if not word:
            return jsonify({"status": "error", "error": "è¯æ±‡ä¸èƒ½ä¸ºç©º"}), 400
        
        result = analyze_word_morphology(word)
        result["status"] = "success"
        
        return jsonify(result)
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
